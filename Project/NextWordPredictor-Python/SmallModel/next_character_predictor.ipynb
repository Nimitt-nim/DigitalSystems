{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "with open('/Users/nimitt/Documents/DigitalSystems/Project/Datasets/advisories.txt','r') as file:\n",
    "    text = file.read()\n",
    "text = text.lower()\n",
    "\n",
    "chars = ['\\x00',\n",
    " '\\t',\n",
    " '\\n',\n",
    " ' ',\n",
    " '!',\n",
    " '\"',\n",
    " '#',\n",
    " '$',\n",
    " '%',\n",
    " '&',\n",
    " \"'\",\n",
    " '(',\n",
    " ')',\n",
    " '*',\n",
    " '+',\n",
    " ',',\n",
    " '-',\n",
    " '.',\n",
    " '/',\n",
    " '0',\n",
    " '1',\n",
    " '2',\n",
    " '3',\n",
    " '4',\n",
    " '5',\n",
    " '6',\n",
    " '7',\n",
    " '8',\n",
    " '9',\n",
    " ':',\n",
    " ';',\n",
    " '<',\n",
    " '=',\n",
    " '>',\n",
    " '?',\n",
    " '@',\n",
    " '[',\n",
    " ']',\n",
    " '^',\n",
    " '_',\n",
    " '`',\n",
    " 'a',\n",
    " 'b',\n",
    " 'c',\n",
    " 'd',\n",
    " 'e',\n",
    " 'f',\n",
    " 'g',\n",
    " 'h',\n",
    " 'i',\n",
    " 'j',\n",
    " 'k',\n",
    " 'l',\n",
    " 'm',\n",
    " 'n',\n",
    " 'o',\n",
    " 'p',\n",
    " 'q',\n",
    " 'r',\n",
    " 's',\n",
    " 't',\n",
    " 'u',\n",
    " 'v',\n",
    " 'w',\n",
    " 'x',\n",
    " 'y',\n",
    " 'z',\n",
    " '~',\n",
    " '\\xa0',\n",
    " 'Â§',\n",
    " 'Â·',\n",
    " 'à¤‚',\n",
    " 'à¤•',\n",
    " 'à¤—',\n",
    " 'à¤¤',\n",
    " 'à¤¥',\n",
    " 'à¤¦',\n",
    " 'à¤§',\n",
    " 'à¤¨',\n",
    " 'à¤ª',\n",
    " 'à¤­',\n",
    " 'à¤¯',\n",
    " 'à¤°',\n",
    " 'à¤¸',\n",
    " 'à¤¾',\n",
    " 'à¤¿',\n",
    " 'à¥€',\n",
    " 'à¥‹',\n",
    " 'à¥Œ',\n",
    " 'à¥',\n",
    " 'â€“',\n",
    " 'â€˜',\n",
    " 'â€™',\n",
    " 'â€œ',\n",
    " 'â€',\n",
    " 'â€ž',\n",
    " 'â€Ÿ',\n",
    " 'â€¢',\n",
    " 'â€¦',\n",
    " '\\u2028',\n",
    " 'â‚¹',\n",
    " 'â†’',\n",
    " 'âˆ‘',\n",
    " 'âˆš',\n",
    " 'â‰¥',\n",
    " 'â–¡',\n",
    " 'â–ª',\n",
    " 'â—‹',\n",
    " 'â—',\n",
    " 'âž¢',\n",
    " '\\uf020',\n",
    " '\\uf02d',\n",
    " '\\uf0a7',\n",
    " '\\uf0b7',\n",
    " '\\uf0d8',\n",
    " '\\uf0fc',\n",
    " 'ï¬€',\n",
    " 'ï¬',\n",
    " 'ï¬ƒ',\n",
    " 'ð¶',\n",
    " 'ð¼',\n",
    " 'ð‘ƒ',\n",
    " 'ð‘†',\n",
    " 'ð‘',\n",
    " 'ð‘”']\n",
    "\n",
    "\n",
    "#  Removing unwanted chars\n",
    "unwanted_chars = chars[68:] + chars[:20]\n",
    "for unwanted_char in unwanted_chars:\n",
    "    if (unwanted_char in ['â€¢',\n",
    "                            'â€¦',\n",
    "                            '\\u2028',\n",
    "                            'â‚¹',\n",
    "                            'â†’',\n",
    "                            'âˆ‘','âˆš','â‰¥','â–¡','â–ª','â—‹','â—','âž¢','\\uf020','\\uf02d','\\uf0a7','\\uf0b7','\\uf0d8','\\uf0fc',]):\n",
    "        text = text.replace(unwanted_char,\"|\")\n",
    "    elif (unwanted_char in [ 'â€˜',\n",
    " 'â€™',\n",
    " 'â€œ',\n",
    " 'â€',\n",
    " 'â€ž',\n",
    " 'â€Ÿ',]):\n",
    "        text = text.replace(unwanted_char,\"'\")\n",
    "    else:\n",
    "        text = text.replace(unwanted_char,\"~\")\n",
    "text = text.replace('\\n',\"~\")\n",
    "text = text.replace('\\t',\"~\")\n",
    "text = text.replace('\\x00',\"~\")\n",
    "text_len = 500\n",
    "text = text[:text_len]\n",
    "# Vocabulary\n",
    "chars = sorted(set(text))   \n",
    "\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# Creating X and Y\n",
    "X, Y = [],[]\n",
    "context = []\n",
    "for j in range(block_size):\n",
    "    context = context + [stoi[text[j]]]\n",
    "\n",
    "for i in range(block_size, len(text)-1):\n",
    "\n",
    "    X_ = np.zeros((block_size,len(chars)))\n",
    "\n",
    "    for j in range(block_size):\n",
    "        X_[j][context[j]] = 1\n",
    "\n",
    "    ch = text[i]\n",
    "    ix = stoi[ch]\n",
    "\n",
    "\n",
    "    X.append(X_)\n",
    "    \n",
    "    context = context[1:] + [ix] \n",
    "    Y.append(context)\n",
    "\n",
    "X = torch.tensor(np.array(X),dtype=torch.float32)\n",
    "Y = torch.tensor(np.array(Y),dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 0,\n",
       " '2': 1,\n",
       " '3': 2,\n",
       " '5': 3,\n",
       " 'a': 4,\n",
       " 'b': 5,\n",
       " 'c': 6,\n",
       " 'd': 7,\n",
       " 'e': 8,\n",
       " 'f': 9,\n",
       " 'g': 10,\n",
       " 'h': 11,\n",
       " 'i': 12,\n",
       " 'k': 13,\n",
       " 'l': 14,\n",
       " 'm': 15,\n",
       " 'n': 16,\n",
       " 'o': 17,\n",
       " 'p': 18,\n",
       " 'r': 19,\n",
       " 's': 20,\n",
       " 't': 21,\n",
       " 'u': 22,\n",
       " 'v': 23,\n",
       " 'w': 24,\n",
       " 'y': 25,\n",
       " '~': 26}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1',\n",
       " 1: '2',\n",
       " 2: '3',\n",
       " 3: '5',\n",
       " 4: 'a',\n",
       " 5: 'b',\n",
       " 6: 'c',\n",
       " 7: 'd',\n",
       " 8: 'e',\n",
       " 9: 'f',\n",
       " 10: 'g',\n",
       " 11: 'h',\n",
       " 12: 'i',\n",
       " 13: 'k',\n",
       " 14: 'l',\n",
       " 15: 'm',\n",
       " 16: 'n',\n",
       " 17: 'o',\n",
       " 18: 'p',\n",
       " 19: 'r',\n",
       " 20: 's',\n",
       " 21: 't',\n",
       " 22: 'u',\n",
       " 23: 'v',\n",
       " 24: 'w',\n",
       " 25: 'y',\n",
       " 26: '~'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Forget Gate\n",
    "        self.wf = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.bf = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "        # Input Gate\n",
    "        self.wi = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.bi = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "        # Candidate Gate\n",
    "        self.wc = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.bc = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "        # Output Gate\n",
    "        self.wo = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.bo = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "        # Final Gate\n",
    "        self.wy = nn.Parameter(torch.Tensor(output_size, hidden_size))\n",
    "        self.by = nn.Parameter(torch.Tensor(output_size, 1))\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        # Initialize weights with Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.wf)\n",
    "        nn.init.xavier_uniform_(self.wi)\n",
    "        nn.init.xavier_uniform_(self.wc)\n",
    "        nn.init.xavier_uniform_(self.wo)\n",
    "        nn.init.xavier_uniform_(self.wy)\n",
    "\n",
    "        # Initialize biases to zeros\n",
    "        nn.init.constant_(self.bf, 0)\n",
    "        nn.init.constant_(self.bi, 0)\n",
    "        nn.init.constant_(self.bc, 0)\n",
    "        nn.init.constant_(self.bo, 0)\n",
    "        nn.init.constant_(self.by, 0)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        outputs = []\n",
    "        seq_length = X.size(0)\n",
    "        hidden_state = torch.zeros(self.hidden_size,1,dtype=torch.float32)\n",
    "        cell_state = torch.zeros(self.hidden_size,1,dtype = torch.float32)\n",
    "\n",
    "        for q in range(seq_length):\n",
    "            concat_input = torch.cat((hidden_state, X[q].unsqueeze(1)), dim=0)\n",
    "            forget_gate = torch.sigmoid(torch.matmul(self.wf, concat_input) + self.bf)\n",
    "            input_gate = torch.sigmoid(torch.matmul(self.wi, concat_input) + self.bi)\n",
    "            candidate_gate = torch.tanh(torch.matmul(self.wc, concat_input) + self.bc)\n",
    "            output_gate = torch.sigmoid(torch.matmul(self.wo, concat_input) + self.bo)\n",
    "\n",
    "            cell_state = forget_gate * cell_state + input_gate * candidate_gate\n",
    "            hidden_state = output_gate * torch.tanh(cell_state)\n",
    "\n",
    "            output = torch.matmul(self.wy, hidden_state) + self.by\n",
    "            outputs.append(output)\n",
    "        outputs = torch.stack(outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        out = [self.forward(x) for x in X]\n",
    "        return torch.stack(out)\n",
    "    \n",
    "    def train(self, X, y, epochs, lr, model_path):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            prediction = self.predict(X)\n",
    "            prediction = prediction.reshape(-1, self.output_size)\n",
    "            target = y.reshape(-1)\n",
    "            loss = criterion(prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "            current_loss = epoch_loss / len(X)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {current_loss:.4f}\")\n",
    "\n",
    "            if (epoch % 100 == 0):\n",
    "                torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': self.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'loss': loss,\n",
    "                            }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 27\n",
    "hidden_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = len(chars)\n",
    "\n",
    "model = LSTM(hidden_size+input_size, hidden_size, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 0.0067\n",
      "Epoch 2/500, Loss: 0.0067\n",
      "Epoch 3/500, Loss: 0.0066\n",
      "Epoch 4/500, Loss: 0.0065\n",
      "Epoch 5/500, Loss: 0.0064\n",
      "Epoch 6/500, Loss: 0.0063\n",
      "Epoch 7/500, Loss: 0.0062\n",
      "Epoch 8/500, Loss: 0.0060\n",
      "Epoch 9/500, Loss: 0.0059\n",
      "Epoch 10/500, Loss: 0.0058\n",
      "Epoch 11/500, Loss: 0.0058\n",
      "Epoch 12/500, Loss: 0.0057\n",
      "Epoch 13/500, Loss: 0.0057\n",
      "Epoch 14/500, Loss: 0.0056\n",
      "Epoch 15/500, Loss: 0.0055\n",
      "Epoch 16/500, Loss: 0.0055\n",
      "Epoch 17/500, Loss: 0.0055\n",
      "Epoch 18/500, Loss: 0.0054\n",
      "Epoch 19/500, Loss: 0.0054\n",
      "Epoch 20/500, Loss: 0.0054\n",
      "Epoch 21/500, Loss: 0.0053\n",
      "Epoch 22/500, Loss: 0.0053\n",
      "Epoch 23/500, Loss: 0.0053\n",
      "Epoch 24/500, Loss: 0.0052\n",
      "Epoch 25/500, Loss: 0.0052\n",
      "Epoch 26/500, Loss: 0.0051\n",
      "Epoch 27/500, Loss: 0.0051\n",
      "Epoch 28/500, Loss: 0.0051\n",
      "Epoch 29/500, Loss: 0.0050\n",
      "Epoch 30/500, Loss: 0.0050\n",
      "Epoch 31/500, Loss: 0.0049\n",
      "Epoch 32/500, Loss: 0.0049\n",
      "Epoch 33/500, Loss: 0.0049\n",
      "Epoch 34/500, Loss: 0.0048\n",
      "Epoch 35/500, Loss: 0.0048\n",
      "Epoch 36/500, Loss: 0.0047\n",
      "Epoch 37/500, Loss: 0.0047\n",
      "Epoch 38/500, Loss: 0.0046\n",
      "Epoch 39/500, Loss: 0.0046\n",
      "Epoch 40/500, Loss: 0.0045\n",
      "Epoch 41/500, Loss: 0.0045\n",
      "Epoch 42/500, Loss: 0.0045\n",
      "Epoch 43/500, Loss: 0.0044\n",
      "Epoch 44/500, Loss: 0.0044\n",
      "Epoch 45/500, Loss: 0.0043\n",
      "Epoch 46/500, Loss: 0.0043\n",
      "Epoch 47/500, Loss: 0.0042\n",
      "Epoch 48/500, Loss: 0.0042\n",
      "Epoch 49/500, Loss: 0.0041\n",
      "Epoch 50/500, Loss: 0.0041\n",
      "Epoch 51/500, Loss: 0.0040\n",
      "Epoch 52/500, Loss: 0.0040\n",
      "Epoch 53/500, Loss: 0.0039\n",
      "Epoch 54/500, Loss: 0.0039\n",
      "Epoch 55/500, Loss: 0.0038\n",
      "Epoch 56/500, Loss: 0.0038\n",
      "Epoch 57/500, Loss: 0.0038\n",
      "Epoch 58/500, Loss: 0.0037\n",
      "Epoch 59/500, Loss: 0.0037\n",
      "Epoch 60/500, Loss: 0.0036\n",
      "Epoch 61/500, Loss: 0.0036\n",
      "Epoch 62/500, Loss: 0.0035\n",
      "Epoch 63/500, Loss: 0.0035\n",
      "Epoch 64/500, Loss: 0.0034\n",
      "Epoch 65/500, Loss: 0.0034\n",
      "Epoch 66/500, Loss: 0.0033\n",
      "Epoch 67/500, Loss: 0.0033\n",
      "Epoch 68/500, Loss: 0.0033\n",
      "Epoch 69/500, Loss: 0.0032\n",
      "Epoch 70/500, Loss: 0.0032\n",
      "Epoch 71/500, Loss: 0.0031\n",
      "Epoch 72/500, Loss: 0.0031\n",
      "Epoch 73/500, Loss: 0.0030\n",
      "Epoch 74/500, Loss: 0.0030\n",
      "Epoch 75/500, Loss: 0.0029\n",
      "Epoch 76/500, Loss: 0.0029\n",
      "Epoch 77/500, Loss: 0.0029\n",
      "Epoch 78/500, Loss: 0.0028\n",
      "Epoch 79/500, Loss: 0.0028\n",
      "Epoch 80/500, Loss: 0.0027\n",
      "Epoch 81/500, Loss: 0.0027\n",
      "Epoch 82/500, Loss: 0.0026\n",
      "Epoch 83/500, Loss: 0.0026\n",
      "Epoch 84/500, Loss: 0.0026\n",
      "Epoch 85/500, Loss: 0.0025\n",
      "Epoch 86/500, Loss: 0.0025\n",
      "Epoch 87/500, Loss: 0.0024\n",
      "Epoch 88/500, Loss: 0.0024\n",
      "Epoch 89/500, Loss: 0.0024\n",
      "Epoch 90/500, Loss: 0.0023\n",
      "Epoch 91/500, Loss: 0.0023\n",
      "Epoch 92/500, Loss: 0.0023\n",
      "Epoch 93/500, Loss: 0.0022\n",
      "Epoch 94/500, Loss: 0.0022\n",
      "Epoch 95/500, Loss: 0.0022\n",
      "Epoch 96/500, Loss: 0.0021\n",
      "Epoch 97/500, Loss: 0.0021\n",
      "Epoch 98/500, Loss: 0.0021\n",
      "Epoch 99/500, Loss: 0.0020\n",
      "Epoch 100/500, Loss: 0.0020\n",
      "Epoch 101/500, Loss: 0.0020\n",
      "Epoch 102/500, Loss: 0.0019\n",
      "Epoch 103/500, Loss: 0.0019\n",
      "Epoch 104/500, Loss: 0.0019\n",
      "Epoch 105/500, Loss: 0.0019\n",
      "Epoch 106/500, Loss: 0.0018\n",
      "Epoch 107/500, Loss: 0.0018\n",
      "Epoch 108/500, Loss: 0.0018\n",
      "Epoch 109/500, Loss: 0.0018\n",
      "Epoch 110/500, Loss: 0.0018\n",
      "Epoch 111/500, Loss: 0.0017\n",
      "Epoch 112/500, Loss: 0.0017\n",
      "Epoch 113/500, Loss: 0.0017\n",
      "Epoch 114/500, Loss: 0.0017\n",
      "Epoch 115/500, Loss: 0.0017\n",
      "Epoch 116/500, Loss: 0.0016\n",
      "Epoch 117/500, Loss: 0.0016\n",
      "Epoch 118/500, Loss: 0.0016\n",
      "Epoch 119/500, Loss: 0.0016\n",
      "Epoch 120/500, Loss: 0.0016\n",
      "Epoch 121/500, Loss: 0.0015\n",
      "Epoch 122/500, Loss: 0.0015\n",
      "Epoch 123/500, Loss: 0.0015\n",
      "Epoch 124/500, Loss: 0.0015\n",
      "Epoch 125/500, Loss: 0.0015\n",
      "Epoch 126/500, Loss: 0.0015\n",
      "Epoch 127/500, Loss: 0.0015\n",
      "Epoch 128/500, Loss: 0.0014\n",
      "Epoch 129/500, Loss: 0.0014\n",
      "Epoch 130/500, Loss: 0.0014\n",
      "Epoch 131/500, Loss: 0.0014\n",
      "Epoch 132/500, Loss: 0.0014\n",
      "Epoch 133/500, Loss: 0.0014\n",
      "Epoch 134/500, Loss: 0.0014\n",
      "Epoch 135/500, Loss: 0.0013\n",
      "Epoch 136/500, Loss: 0.0013\n",
      "Epoch 137/500, Loss: 0.0013\n",
      "Epoch 138/500, Loss: 0.0013\n",
      "Epoch 139/500, Loss: 0.0013\n",
      "Epoch 140/500, Loss: 0.0013\n",
      "Epoch 141/500, Loss: 0.0013\n",
      "Epoch 142/500, Loss: 0.0013\n",
      "Epoch 143/500, Loss: 0.0013\n",
      "Epoch 144/500, Loss: 0.0012\n",
      "Epoch 145/500, Loss: 0.0012\n",
      "Epoch 146/500, Loss: 0.0012\n",
      "Epoch 147/500, Loss: 0.0012\n",
      "Epoch 148/500, Loss: 0.0012\n",
      "Epoch 149/500, Loss: 0.0012\n",
      "Epoch 150/500, Loss: 0.0012\n",
      "Epoch 151/500, Loss: 0.0012\n",
      "Epoch 152/500, Loss: 0.0012\n",
      "Epoch 153/500, Loss: 0.0012\n",
      "Epoch 154/500, Loss: 0.0012\n",
      "Epoch 155/500, Loss: 0.0011\n",
      "Epoch 156/500, Loss: 0.0011\n",
      "Epoch 157/500, Loss: 0.0011\n",
      "Epoch 158/500, Loss: 0.0011\n",
      "Epoch 159/500, Loss: 0.0011\n",
      "Epoch 160/500, Loss: 0.0011\n",
      "Epoch 161/500, Loss: 0.0011\n",
      "Epoch 162/500, Loss: 0.0011\n",
      "Epoch 163/500, Loss: 0.0011\n",
      "Epoch 164/500, Loss: 0.0011\n",
      "Epoch 165/500, Loss: 0.0011\n",
      "Epoch 166/500, Loss: 0.0011\n",
      "Epoch 167/500, Loss: 0.0011\n",
      "Epoch 168/500, Loss: 0.0011\n",
      "Epoch 169/500, Loss: 0.0011\n",
      "Epoch 170/500, Loss: 0.0010\n",
      "Epoch 171/500, Loss: 0.0010\n",
      "Epoch 172/500, Loss: 0.0010\n",
      "Epoch 173/500, Loss: 0.0010\n",
      "Epoch 174/500, Loss: 0.0010\n",
      "Epoch 175/500, Loss: 0.0010\n",
      "Epoch 176/500, Loss: 0.0010\n",
      "Epoch 177/500, Loss: 0.0010\n",
      "Epoch 178/500, Loss: 0.0010\n",
      "Epoch 179/500, Loss: 0.0010\n",
      "Epoch 180/500, Loss: 0.0010\n",
      "Epoch 181/500, Loss: 0.0010\n",
      "Epoch 182/500, Loss: 0.0010\n",
      "Epoch 183/500, Loss: 0.0010\n",
      "Epoch 184/500, Loss: 0.0010\n",
      "Epoch 185/500, Loss: 0.0010\n",
      "Epoch 186/500, Loss: 0.0010\n",
      "Epoch 187/500, Loss: 0.0010\n",
      "Epoch 188/500, Loss: 0.0010\n",
      "Epoch 189/500, Loss: 0.0010\n",
      "Epoch 190/500, Loss: 0.0010\n",
      "Epoch 191/500, Loss: 0.0009\n",
      "Epoch 192/500, Loss: 0.0009\n",
      "Epoch 193/500, Loss: 0.0009\n",
      "Epoch 194/500, Loss: 0.0009\n",
      "Epoch 195/500, Loss: 0.0009\n",
      "Epoch 196/500, Loss: 0.0009\n",
      "Epoch 197/500, Loss: 0.0009\n",
      "Epoch 198/500, Loss: 0.0009\n",
      "Epoch 199/500, Loss: 0.0009\n",
      "Epoch 200/500, Loss: 0.0009\n",
      "Epoch 201/500, Loss: 0.0009\n",
      "Epoch 202/500, Loss: 0.0009\n",
      "Epoch 203/500, Loss: 0.0009\n",
      "Epoch 204/500, Loss: 0.0009\n",
      "Epoch 205/500, Loss: 0.0009\n",
      "Epoch 206/500, Loss: 0.0009\n",
      "Epoch 207/500, Loss: 0.0009\n",
      "Epoch 208/500, Loss: 0.0009\n",
      "Epoch 209/500, Loss: 0.0009\n",
      "Epoch 210/500, Loss: 0.0009\n",
      "Epoch 211/500, Loss: 0.0009\n",
      "Epoch 212/500, Loss: 0.0009\n",
      "Epoch 213/500, Loss: 0.0009\n",
      "Epoch 214/500, Loss: 0.0009\n",
      "Epoch 215/500, Loss: 0.0009\n",
      "Epoch 216/500, Loss: 0.0009\n",
      "Epoch 217/500, Loss: 0.0009\n",
      "Epoch 218/500, Loss: 0.0009\n",
      "Epoch 219/500, Loss: 0.0009\n",
      "Epoch 220/500, Loss: 0.0009\n",
      "Epoch 221/500, Loss: 0.0009\n",
      "Epoch 222/500, Loss: 0.0009\n",
      "Epoch 223/500, Loss: 0.0009\n",
      "Epoch 224/500, Loss: 0.0009\n",
      "Epoch 225/500, Loss: 0.0008\n",
      "Epoch 226/500, Loss: 0.0008\n",
      "Epoch 227/500, Loss: 0.0008\n",
      "Epoch 228/500, Loss: 0.0008\n",
      "Epoch 229/500, Loss: 0.0008\n",
      "Epoch 230/500, Loss: 0.0008\n",
      "Epoch 231/500, Loss: 0.0008\n",
      "Epoch 232/500, Loss: 0.0008\n",
      "Epoch 233/500, Loss: 0.0008\n",
      "Epoch 234/500, Loss: 0.0008\n",
      "Epoch 235/500, Loss: 0.0008\n",
      "Epoch 236/500, Loss: 0.0008\n",
      "Epoch 237/500, Loss: 0.0008\n",
      "Epoch 238/500, Loss: 0.0008\n",
      "Epoch 239/500, Loss: 0.0008\n",
      "Epoch 240/500, Loss: 0.0008\n",
      "Epoch 241/500, Loss: 0.0008\n",
      "Epoch 242/500, Loss: 0.0008\n",
      "Epoch 243/500, Loss: 0.0008\n",
      "Epoch 244/500, Loss: 0.0008\n",
      "Epoch 245/500, Loss: 0.0008\n",
      "Epoch 246/500, Loss: 0.0008\n",
      "Epoch 247/500, Loss: 0.0008\n",
      "Epoch 248/500, Loss: 0.0008\n",
      "Epoch 249/500, Loss: 0.0008\n",
      "Epoch 250/500, Loss: 0.0008\n",
      "Epoch 251/500, Loss: 0.0008\n",
      "Epoch 252/500, Loss: 0.0008\n",
      "Epoch 253/500, Loss: 0.0008\n",
      "Epoch 254/500, Loss: 0.0008\n",
      "Epoch 255/500, Loss: 0.0008\n",
      "Epoch 256/500, Loss: 0.0008\n",
      "Epoch 257/500, Loss: 0.0008\n",
      "Epoch 258/500, Loss: 0.0008\n",
      "Epoch 259/500, Loss: 0.0008\n",
      "Epoch 260/500, Loss: 0.0008\n",
      "Epoch 261/500, Loss: 0.0008\n",
      "Epoch 262/500, Loss: 0.0008\n",
      "Epoch 263/500, Loss: 0.0008\n",
      "Epoch 264/500, Loss: 0.0008\n",
      "Epoch 265/500, Loss: 0.0008\n",
      "Epoch 266/500, Loss: 0.0008\n",
      "Epoch 267/500, Loss: 0.0008\n",
      "Epoch 268/500, Loss: 0.0008\n",
      "Epoch 269/500, Loss: 0.0008\n",
      "Epoch 270/500, Loss: 0.0008\n",
      "Epoch 271/500, Loss: 0.0008\n",
      "Epoch 272/500, Loss: 0.0008\n",
      "Epoch 273/500, Loss: 0.0008\n",
      "Epoch 274/500, Loss: 0.0008\n",
      "Epoch 275/500, Loss: 0.0008\n",
      "Epoch 276/500, Loss: 0.0008\n",
      "Epoch 277/500, Loss: 0.0008\n",
      "Epoch 278/500, Loss: 0.0008\n",
      "Epoch 279/500, Loss: 0.0008\n",
      "Epoch 280/500, Loss: 0.0008\n",
      "Epoch 281/500, Loss: 0.0008\n",
      "Epoch 282/500, Loss: 0.0008\n",
      "Epoch 283/500, Loss: 0.0008\n",
      "Epoch 284/500, Loss: 0.0008\n",
      "Epoch 285/500, Loss: 0.0008\n",
      "Epoch 286/500, Loss: 0.0008\n",
      "Epoch 287/500, Loss: 0.0008\n",
      "Epoch 288/500, Loss: 0.0008\n",
      "Epoch 289/500, Loss: 0.0008\n",
      "Epoch 290/500, Loss: 0.0008\n",
      "Epoch 291/500, Loss: 0.0008\n",
      "Epoch 292/500, Loss: 0.0008\n",
      "Epoch 293/500, Loss: 0.0008\n",
      "Epoch 294/500, Loss: 0.0008\n",
      "Epoch 295/500, Loss: 0.0008\n",
      "Epoch 296/500, Loss: 0.0008\n",
      "Epoch 297/500, Loss: 0.0008\n",
      "Epoch 298/500, Loss: 0.0008\n",
      "Epoch 299/500, Loss: 0.0008\n",
      "Epoch 300/500, Loss: 0.0008\n",
      "Epoch 301/500, Loss: 0.0007\n",
      "Epoch 302/500, Loss: 0.0007\n",
      "Epoch 303/500, Loss: 0.0007\n",
      "Epoch 304/500, Loss: 0.0007\n",
      "Epoch 305/500, Loss: 0.0007\n",
      "Epoch 306/500, Loss: 0.0007\n",
      "Epoch 307/500, Loss: 0.0007\n",
      "Epoch 308/500, Loss: 0.0007\n",
      "Epoch 309/500, Loss: 0.0007\n",
      "Epoch 310/500, Loss: 0.0007\n",
      "Epoch 311/500, Loss: 0.0007\n",
      "Epoch 312/500, Loss: 0.0007\n",
      "Epoch 313/500, Loss: 0.0007\n",
      "Epoch 314/500, Loss: 0.0007\n",
      "Epoch 315/500, Loss: 0.0007\n",
      "Epoch 316/500, Loss: 0.0007\n",
      "Epoch 317/500, Loss: 0.0007\n",
      "Epoch 318/500, Loss: 0.0007\n",
      "Epoch 319/500, Loss: 0.0007\n",
      "Epoch 320/500, Loss: 0.0007\n",
      "Epoch 321/500, Loss: 0.0007\n",
      "Epoch 322/500, Loss: 0.0007\n",
      "Epoch 323/500, Loss: 0.0007\n",
      "Epoch 324/500, Loss: 0.0007\n",
      "Epoch 325/500, Loss: 0.0007\n",
      "Epoch 326/500, Loss: 0.0007\n",
      "Epoch 327/500, Loss: 0.0007\n",
      "Epoch 328/500, Loss: 0.0007\n",
      "Epoch 329/500, Loss: 0.0007\n",
      "Epoch 330/500, Loss: 0.0007\n",
      "Epoch 331/500, Loss: 0.0007\n",
      "Epoch 332/500, Loss: 0.0007\n",
      "Epoch 333/500, Loss: 0.0007\n",
      "Epoch 334/500, Loss: 0.0007\n",
      "Epoch 335/500, Loss: 0.0007\n",
      "Epoch 336/500, Loss: 0.0007\n",
      "Epoch 337/500, Loss: 0.0007\n",
      "Epoch 338/500, Loss: 0.0007\n",
      "Epoch 339/500, Loss: 0.0007\n",
      "Epoch 340/500, Loss: 0.0007\n",
      "Epoch 341/500, Loss: 0.0007\n",
      "Epoch 342/500, Loss: 0.0007\n",
      "Epoch 343/500, Loss: 0.0007\n",
      "Epoch 344/500, Loss: 0.0007\n",
      "Epoch 345/500, Loss: 0.0007\n",
      "Epoch 346/500, Loss: 0.0007\n",
      "Epoch 347/500, Loss: 0.0007\n",
      "Epoch 348/500, Loss: 0.0007\n",
      "Epoch 349/500, Loss: 0.0007\n",
      "Epoch 350/500, Loss: 0.0007\n",
      "Epoch 351/500, Loss: 0.0007\n",
      "Epoch 352/500, Loss: 0.0007\n",
      "Epoch 353/500, Loss: 0.0007\n",
      "Epoch 354/500, Loss: 0.0007\n",
      "Epoch 355/500, Loss: 0.0007\n",
      "Epoch 356/500, Loss: 0.0007\n",
      "Epoch 357/500, Loss: 0.0007\n",
      "Epoch 358/500, Loss: 0.0007\n",
      "Epoch 359/500, Loss: 0.0007\n",
      "Epoch 360/500, Loss: 0.0007\n",
      "Epoch 361/500, Loss: 0.0007\n",
      "Epoch 362/500, Loss: 0.0007\n",
      "Epoch 363/500, Loss: 0.0007\n",
      "Epoch 364/500, Loss: 0.0007\n",
      "Epoch 365/500, Loss: 0.0007\n",
      "Epoch 366/500, Loss: 0.0007\n",
      "Epoch 367/500, Loss: 0.0007\n",
      "Epoch 368/500, Loss: 0.0007\n",
      "Epoch 369/500, Loss: 0.0007\n",
      "Epoch 370/500, Loss: 0.0007\n",
      "Epoch 371/500, Loss: 0.0007\n",
      "Epoch 372/500, Loss: 0.0007\n",
      "Epoch 373/500, Loss: 0.0007\n",
      "Epoch 374/500, Loss: 0.0007\n",
      "Epoch 375/500, Loss: 0.0007\n",
      "Epoch 376/500, Loss: 0.0007\n",
      "Epoch 377/500, Loss: 0.0007\n",
      "Epoch 378/500, Loss: 0.0007\n",
      "Epoch 379/500, Loss: 0.0007\n",
      "Epoch 380/500, Loss: 0.0007\n",
      "Epoch 381/500, Loss: 0.0007\n",
      "Epoch 382/500, Loss: 0.0007\n",
      "Epoch 383/500, Loss: 0.0007\n",
      "Epoch 384/500, Loss: 0.0007\n",
      "Epoch 385/500, Loss: 0.0007\n",
      "Epoch 386/500, Loss: 0.0007\n",
      "Epoch 387/500, Loss: 0.0007\n",
      "Epoch 388/500, Loss: 0.0007\n",
      "Epoch 389/500, Loss: 0.0007\n",
      "Epoch 390/500, Loss: 0.0007\n",
      "Epoch 391/500, Loss: 0.0007\n",
      "Epoch 392/500, Loss: 0.0007\n",
      "Epoch 393/500, Loss: 0.0007\n",
      "Epoch 394/500, Loss: 0.0007\n",
      "Epoch 395/500, Loss: 0.0007\n",
      "Epoch 396/500, Loss: 0.0007\n",
      "Epoch 397/500, Loss: 0.0007\n",
      "Epoch 398/500, Loss: 0.0007\n",
      "Epoch 399/500, Loss: 0.0007\n",
      "Epoch 400/500, Loss: 0.0007\n",
      "Epoch 401/500, Loss: 0.0007\n",
      "Epoch 402/500, Loss: 0.0007\n",
      "Epoch 403/500, Loss: 0.0007\n",
      "Epoch 404/500, Loss: 0.0007\n",
      "Epoch 405/500, Loss: 0.0007\n",
      "Epoch 406/500, Loss: 0.0007\n",
      "Epoch 407/500, Loss: 0.0007\n",
      "Epoch 408/500, Loss: 0.0007\n",
      "Epoch 409/500, Loss: 0.0007\n",
      "Epoch 410/500, Loss: 0.0007\n",
      "Epoch 411/500, Loss: 0.0007\n",
      "Epoch 412/500, Loss: 0.0007\n",
      "Epoch 413/500, Loss: 0.0007\n",
      "Epoch 414/500, Loss: 0.0007\n",
      "Epoch 415/500, Loss: 0.0007\n",
      "Epoch 416/500, Loss: 0.0007\n",
      "Epoch 417/500, Loss: 0.0007\n",
      "Epoch 418/500, Loss: 0.0007\n",
      "Epoch 419/500, Loss: 0.0007\n",
      "Epoch 420/500, Loss: 0.0007\n",
      "Epoch 421/500, Loss: 0.0007\n",
      "Epoch 422/500, Loss: 0.0007\n",
      "Epoch 423/500, Loss: 0.0007\n",
      "Epoch 424/500, Loss: 0.0007\n",
      "Epoch 425/500, Loss: 0.0007\n",
      "Epoch 426/500, Loss: 0.0007\n",
      "Epoch 427/500, Loss: 0.0007\n",
      "Epoch 428/500, Loss: 0.0007\n",
      "Epoch 429/500, Loss: 0.0007\n",
      "Epoch 430/500, Loss: 0.0007\n",
      "Epoch 431/500, Loss: 0.0007\n",
      "Epoch 432/500, Loss: 0.0007\n",
      "Epoch 433/500, Loss: 0.0007\n",
      "Epoch 434/500, Loss: 0.0007\n",
      "Epoch 435/500, Loss: 0.0007\n",
      "Epoch 436/500, Loss: 0.0007\n",
      "Epoch 437/500, Loss: 0.0007\n",
      "Epoch 438/500, Loss: 0.0007\n",
      "Epoch 439/500, Loss: 0.0007\n",
      "Epoch 440/500, Loss: 0.0007\n",
      "Epoch 441/500, Loss: 0.0007\n",
      "Epoch 442/500, Loss: 0.0007\n",
      "Epoch 443/500, Loss: 0.0007\n",
      "Epoch 444/500, Loss: 0.0007\n",
      "Epoch 445/500, Loss: 0.0007\n",
      "Epoch 446/500, Loss: 0.0007\n",
      "Epoch 447/500, Loss: 0.0007\n",
      "Epoch 448/500, Loss: 0.0007\n",
      "Epoch 449/500, Loss: 0.0007\n",
      "Epoch 450/500, Loss: 0.0007\n",
      "Epoch 451/500, Loss: 0.0007\n",
      "Epoch 452/500, Loss: 0.0007\n",
      "Epoch 453/500, Loss: 0.0007\n",
      "Epoch 454/500, Loss: 0.0007\n",
      "Epoch 455/500, Loss: 0.0007\n",
      "Epoch 456/500, Loss: 0.0007\n",
      "Epoch 457/500, Loss: 0.0007\n",
      "Epoch 458/500, Loss: 0.0007\n",
      "Epoch 459/500, Loss: 0.0007\n",
      "Epoch 460/500, Loss: 0.0007\n",
      "Epoch 461/500, Loss: 0.0007\n",
      "Epoch 462/500, Loss: 0.0007\n",
      "Epoch 463/500, Loss: 0.0007\n",
      "Epoch 464/500, Loss: 0.0007\n",
      "Epoch 465/500, Loss: 0.0007\n",
      "Epoch 466/500, Loss: 0.0007\n",
      "Epoch 467/500, Loss: 0.0007\n",
      "Epoch 468/500, Loss: 0.0007\n",
      "Epoch 469/500, Loss: 0.0007\n",
      "Epoch 470/500, Loss: 0.0007\n",
      "Epoch 471/500, Loss: 0.0007\n",
      "Epoch 472/500, Loss: 0.0007\n",
      "Epoch 473/500, Loss: 0.0007\n",
      "Epoch 474/500, Loss: 0.0007\n",
      "Epoch 475/500, Loss: 0.0007\n",
      "Epoch 476/500, Loss: 0.0007\n",
      "Epoch 477/500, Loss: 0.0007\n",
      "Epoch 478/500, Loss: 0.0007\n",
      "Epoch 479/500, Loss: 0.0007\n",
      "Epoch 480/500, Loss: 0.0007\n",
      "Epoch 481/500, Loss: 0.0007\n",
      "Epoch 482/500, Loss: 0.0007\n",
      "Epoch 483/500, Loss: 0.0007\n",
      "Epoch 484/500, Loss: 0.0007\n",
      "Epoch 485/500, Loss: 0.0007\n",
      "Epoch 486/500, Loss: 0.0007\n",
      "Epoch 487/500, Loss: 0.0007\n",
      "Epoch 488/500, Loss: 0.0007\n",
      "Epoch 489/500, Loss: 0.0007\n",
      "Epoch 490/500, Loss: 0.0007\n",
      "Epoch 491/500, Loss: 0.0007\n",
      "Epoch 492/500, Loss: 0.0007\n",
      "Epoch 493/500, Loss: 0.0007\n",
      "Epoch 494/500, Loss: 0.0007\n",
      "Epoch 495/500, Loss: 0.0007\n",
      "Epoch 496/500, Loss: 0.0007\n",
      "Epoch 497/500, Loss: 0.0007\n",
      "Epoch 498/500, Loss: 0.0007\n",
      "Epoch 499/500, Loss: 0.0007\n",
      "Epoch 500/500, Loss: 0.0007\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# checkpoint_folder = \"/Users/nimitt/Documents/DigitalSystems/Project/Model_States\"\n",
    "# model_filename = f\"model_{block_size}_{hidden_size}_{text_len}.pt\"\n",
    "# model_path = os.path.join(checkpoint_folder, model_filename)\n",
    "# # Ensure the folder exists\n",
    "# os.makedirs(checkpoint_folder, exist_ok=True)\n",
    "\n",
    "# model.train(X,Y,500,0.01,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alumni                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "# # Testing\n",
    "\n",
    "# def convert_to_X(prompt):\n",
    "#     X_ = np.zeros((len(prompt),input_size))\n",
    "#     for i in range(len(prompt)):\n",
    "#         X_[i][stoi[prompt[i]]] = 1\n",
    "#     return torch.tensor(X_,dtype = torch.float32)\n",
    "        \n",
    "# prompt = \"alumni\"\n",
    "# max_len = 100\n",
    "# context = []\n",
    "# for j in range(len(prompt)):\n",
    "#     context = context + [stoi[prompt[j]]]\n",
    "# context = context[-block_size:]\n",
    "\n",
    "# generated_text = prompt\n",
    "# for i in range(max_len):\n",
    "#     x = convert_to_X(generated_text)\n",
    "#     y_pred = model(x)[-1]\n",
    "#     # ix = torch.distributions.categorical.Categorical(logits=y_pred.squeeze()).sample()\n",
    "#     ix = torch.argmax(y_pred)\n",
    "#     ch = itos[ix.item()]\n",
    "#     generated_text += ch\n",
    "#     context = context[1:] + [ix]\n",
    "\n",
    "# genrated_text = generated_text.replace('|','\\n')\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alumni relations advisory 02 (version 1.0, december 2021)                                 page 1 of 1 ~ indian institute of technology gandhinagar  ~ ~honorary  alumni program at iit gandhinagar  ~(as approved by the bog in its 33rd meeting held on 15 november 2021)  ~ ~ ~the board of governors of the institute in its 33rd meeting held on 15 november ~2021 approved the following honorary alumni program  at iit gandhinagar.  ~  ~1. individuals who are not graduates of iit gandhinagar and who make'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| basiapproved~the~following~honorary~~alumni~program~at |\n"
     ]
    }
   ],
   "source": [
    "model_read = LSTM(input_size+hidden_size, hidden_size, input_size)\n",
    "model_path = \"/Users/nimitt/Documents/DigitalSystems/Project/NextWordPredictor-Python/SmallModelStates/model_10_25_500.pt\"\n",
    "checkpoint = torch.load(model_path,map_location=torch.device('cpu'))\n",
    "model_read.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Testing\n",
    "\n",
    "def convert_to_X(prompt):\n",
    "    X_ = np.zeros((len(prompt),input_size))\n",
    "    for i in range(len(prompt)):\n",
    "        X_[i][stoi[prompt[i]]] = 1\n",
    "    return torch.tensor(X_,dtype = torch.float32)\n",
    "        \n",
    "prompt = \"basi\"\n",
    "max_len = 50\n",
    "context = []\n",
    "for j in range(len(prompt)):\n",
    "    context = context + [stoi[prompt[j]]]\n",
    "context = context[-block_size:]\n",
    "\n",
    "generated_text = prompt\n",
    "for i in range(max_len):\n",
    "    x = convert_to_X(generated_text)\n",
    "    y_pred = model_read(x)[-1]\n",
    "    # ix = torch.distributions.categorical.Categorical(logits=y_pred.squeeze()).sample()\n",
    "    ix = torch.argmax(y_pred)\n",
    "    ch = itos[ix.item()]\n",
    "    generated_text += ch\n",
    "    context = context[1:] + [ix]\n",
    "\n",
    "genrated_text = generated_text.replace('|','\\n')\n",
    "print(\"|\",generated_text,\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# Writing the Parameters ##################\n",
    "parameter_list = [model_read.wf, \n",
    "model_read.bf,\n",
    "model_read.wi, \n",
    "model_read.bi,\n",
    "model_read.wc, \n",
    "model_read.bc,\n",
    "model_read.wo, \n",
    "model_read.bo, \n",
    "model_read.wy, \n",
    "model_read.by]\n",
    "\n",
    "parameter_names = ['wf','bf','wi','bi','wc','bc','wo','bo','wy','by']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "parameter_list = [model_read.wf, model_read.bf, model_read.wi, model_read.bi, model_read.wc,\n",
    "                  model_read.bc, model_read.wo, model_read.bo, model_read.wy, model_read.by]\n",
    "\n",
    "parameter_names = ['wf', 'bf', 'wi', 'bi', 'wc', 'bc', 'wo', 'bo', 'wy', 'by']\n",
    "\n",
    "with open(\"/Users/nimitt/Documents/DigitalSystems/Project/NextWordPredictor-Python/SmallModelStates/parameters_10_25.txt\", 'w') as file:\n",
    "    file.write(\"Model Parameters\\n\")\n",
    "    file.write(\"================\\n\\n\")\n",
    "    for name, parameter in zip(parameter_names, parameter_list):\n",
    "        file.write(name + \":\\n\")\n",
    "        # Convert tensor to NumPy array\n",
    "        parameter_array = parameter.detach().numpy()\n",
    "        # Write each value of the parameter tensor to the file\n",
    "        for row in parameter_array:\n",
    "            row_str = \" \".join([str(val) for val in row])\n",
    "            file.write(row_str + \"\\n\")\n",
    "        file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/nimitt/Documents/DigitalSystems/Project/NextWordPredictor-Python/SmallModelStates/parameters_10_25.txt\",'r') as file: \n",
    "    params_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_data = params_data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'wf':[],'bf':[],'wi':[],'bi':[],'wc':[],'bc':[],'wo':[],'bo':[],'wy':[],'by':[]}\n",
    "current_parameter = ''\n",
    "for i in range(3, len(params_data)):\n",
    "    if (params_data[i] == ''):\n",
    "        continue\n",
    "    if (params_data[i][:2] in parameters.keys()):\n",
    "        current_parameter = params_data[i][:2]\n",
    "        continue\n",
    "    parameters[current_parameter].append(list(map(float,params_data[i].split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = parameters['wf']\n",
    "bf = parameters['bf']\n",
    "wi = parameters['wi']\n",
    "bi= parameters['bi']\n",
    "wc = parameters['wc']\n",
    "bc= parameters['bc']\n",
    "wo = parameters['wo']\n",
    "bo = parameters['bo']\n",
    "wy = parameters['wy']\n",
    "by= parameters['by']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ converting parameters to 32bit IEEE 754 #####################\n",
    "def float_to_binary_IEEE754(f):\n",
    "    packed = struct.pack('>f', f)\n",
    "    binary = ''.join(format(byte, '08b') for byte in packed)\n",
    "    \n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [wf,bf,wi,bi,wo,bo,wc,bc,wy,by]\n",
    "parameters_flatted = []\n",
    "for parameter in parameters:\n",
    "    for i in range(len(parameter)):\n",
    "        for j in range(len(parameter[0])):\n",
    "            parameters_flatted.append(float_to_binary_IEEE754(parameter[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6002"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parameters_flatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/nimitt/Documents/DigitalSystems/Project/NextWordPredictor-Python/SmallModelStates/parameters_10_25_ieee.txt\",'w') as file:\n",
    "    for i in range(len(parameters_flatted)):\n",
    "        file.write(parameters_flatted[i])\n",
    "        file.write(\",\")\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 52\n",
      "25 1\n",
      "25 52\n",
      "25 1\n",
      "25 52\n",
      "25 1\n",
      "25 52\n",
      "25 1\n",
      "27 25\n",
      "27 1\n"
     ]
    }
   ],
   "source": [
    "for parameter in parameters:\n",
    "    print(len(parameter),len(parameter[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
