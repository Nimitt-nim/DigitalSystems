{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "with open('/Users/nimitt/Documents/DigitalSystems/Project/Datasets/advisories.txt','r') as file:\n",
    "    text = file.read()\n",
    "text = text.lower()\n",
    "\n",
    "chars = ['\\x00',\n",
    " '\\t',\n",
    " '\\n',\n",
    " ' ',\n",
    " '!',\n",
    " '\"',\n",
    " '#',\n",
    " '$',\n",
    " '%',\n",
    " '&',\n",
    " \"'\",\n",
    " '(',\n",
    " ')',\n",
    " '*',\n",
    " '+',\n",
    " ',',\n",
    " '-',\n",
    " '.',\n",
    " '/',\n",
    " '0',\n",
    " '1',\n",
    " '2',\n",
    " '3',\n",
    " '4',\n",
    " '5',\n",
    " '6',\n",
    " '7',\n",
    " '8',\n",
    " '9',\n",
    " ':',\n",
    " ';',\n",
    " '<',\n",
    " '=',\n",
    " '>',\n",
    " '?',\n",
    " '@',\n",
    " '[',\n",
    " ']',\n",
    " '^',\n",
    " '_',\n",
    " '`',\n",
    " 'a',\n",
    " 'b',\n",
    " 'c',\n",
    " 'd',\n",
    " 'e',\n",
    " 'f',\n",
    " 'g',\n",
    " 'h',\n",
    " 'i',\n",
    " 'j',\n",
    " 'k',\n",
    " 'l',\n",
    " 'm',\n",
    " 'n',\n",
    " 'o',\n",
    " 'p',\n",
    " 'q',\n",
    " 'r',\n",
    " 's',\n",
    " 't',\n",
    " 'u',\n",
    " 'v',\n",
    " 'w',\n",
    " 'x',\n",
    " 'y',\n",
    " 'z',\n",
    " '~',\n",
    " '\\xa0',\n",
    " '¬ß',\n",
    " '¬∑',\n",
    " '‡§Ç',\n",
    " '‡§ï',\n",
    " '‡§ó',\n",
    " '‡§§',\n",
    " '‡§•',\n",
    " '‡§¶',\n",
    " '‡§ß',\n",
    " '‡§®',\n",
    " '‡§™',\n",
    " '‡§≠',\n",
    " '‡§Ø',\n",
    " '‡§∞',\n",
    " '‡§∏',\n",
    " '‡§æ',\n",
    " '‡§ø',\n",
    " '‡•Ä',\n",
    " '‡•ã',\n",
    " '‡•å',\n",
    " '‡•ç',\n",
    " '‚Äì',\n",
    " '‚Äò',\n",
    " '‚Äô',\n",
    " '‚Äú',\n",
    " '‚Äù',\n",
    " '‚Äû',\n",
    " '‚Äü',\n",
    " '‚Ä¢',\n",
    " '‚Ä¶',\n",
    " '\\u2028',\n",
    " '‚Çπ',\n",
    " '‚Üí',\n",
    " '‚àë',\n",
    " '‚àö',\n",
    " '‚â•',\n",
    " '‚ñ°',\n",
    " '‚ñ™',\n",
    " '‚óã',\n",
    " '‚óè',\n",
    " '‚û¢',\n",
    " '\\uf020',\n",
    " '\\uf02d',\n",
    " '\\uf0a7',\n",
    " '\\uf0b7',\n",
    " '\\uf0d8',\n",
    " '\\uf0fc',\n",
    " 'Ô¨Ä',\n",
    " 'Ô¨Å',\n",
    " 'Ô¨É',\n",
    " 'ùê∂',\n",
    " 'ùêº',\n",
    " 'ùëÉ',\n",
    " 'ùëÜ',\n",
    " 'ùëê',\n",
    " 'ùëî']\n",
    "\n",
    "#  Removing unwanted chars\n",
    "unwanted_chars = chars[68:]\n",
    "for unwanted_char in unwanted_chars:\n",
    "    if (unwanted_char in ['‚Ä¢',\n",
    "                            '‚Ä¶',\n",
    "                            '\\u2028',\n",
    "                            '‚Çπ',\n",
    "                            '‚Üí',\n",
    "                            '‚àë','‚àö','‚â•','‚ñ°','‚ñ™','‚óã','‚óè','‚û¢','\\uf020','\\uf02d','\\uf0a7','\\uf0b7','\\uf0d8','\\uf0fc',]):\n",
    "        text = text.replace(unwanted_char,\"|\")\n",
    "    elif (unwanted_char in [ '‚Äò',\n",
    " '‚Äô',\n",
    " '‚Äú',\n",
    " '‚Äù',\n",
    " '‚Äû',\n",
    " '‚Äü',]):\n",
    "        text = text.replace(unwanted_char,\"'\")\n",
    "    else:\n",
    "        text = text.replace(unwanted_char,\"~\")\n",
    "text = text.replace('\\n',\"~\")\n",
    "text = text.replace('\\t',\"~\")\n",
    "text = text.replace('\\x00',\"~\")\n",
    "text_len = 5000\n",
    "text = text[:text_len]\n",
    "# Vocabulary\n",
    "chars = sorted(set(text))   \n",
    "\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# Creating X and Y\n",
    "X, Y = [],[]\n",
    "context = []\n",
    "for j in range(block_size):\n",
    "    context = context + [stoi[text[j]]]\n",
    "\n",
    "for i in range(block_size, len(text)-1):\n",
    "\n",
    "    X_ = np.zeros((block_size,len(chars)))\n",
    "\n",
    "    for j in range(block_size):\n",
    "        X_[j][context[j]] = 1\n",
    "\n",
    "    ch = text[i]\n",
    "    ix = stoi[ch]\n",
    "\n",
    "\n",
    "    X.append(X_)\n",
    "    \n",
    "    context = context[1:] + [ix] \n",
    "    Y.append(context)\n",
    "\n",
    "X = torch.tensor(np.array(X),dtype=torch.float32)\n",
    "Y = torch.tensor(np.array(Y),dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '&': 1,\n",
       " \"'\": 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " ',': 5,\n",
       " '-': 6,\n",
       " '.': 7,\n",
       " '/': 8,\n",
       " '0': 9,\n",
       " '1': 10,\n",
       " '2': 11,\n",
       " '3': 12,\n",
       " '4': 13,\n",
       " '5': 14,\n",
       " '6': 15,\n",
       " '7': 16,\n",
       " '8': 17,\n",
       " '9': 18,\n",
       " ':': 19,\n",
       " ';': 20,\n",
       " '[': 21,\n",
       " ']': 22,\n",
       " 'a': 23,\n",
       " 'b': 24,\n",
       " 'c': 25,\n",
       " 'd': 26,\n",
       " 'e': 27,\n",
       " 'f': 28,\n",
       " 'g': 29,\n",
       " 'h': 30,\n",
       " 'i': 31,\n",
       " 'j': 32,\n",
       " 'k': 33,\n",
       " 'l': 34,\n",
       " 'm': 35,\n",
       " 'n': 36,\n",
       " 'o': 37,\n",
       " 'p': 38,\n",
       " 'q': 39,\n",
       " 'r': 40,\n",
       " 's': 41,\n",
       " 't': 42,\n",
       " 'u': 43,\n",
       " 'v': 44,\n",
       " 'w': 45,\n",
       " 'x': 46,\n",
       " 'y': 47,\n",
       " 'z': 48,\n",
       " '|': 49,\n",
       " '~': 50}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Forget Gate\n",
    "        self.wf = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.bf = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "        # Input Gate\n",
    "        self.wi = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.bi = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "        # Candidate Gate\n",
    "        self.wc = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.bc = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "        # Output Gate\n",
    "        self.wo = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.bo = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "        # Final Gate\n",
    "        self.wy = nn.Parameter(torch.Tensor(output_size, hidden_size))\n",
    "        self.by = nn.Parameter(torch.Tensor(output_size, 1))\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        # Initialize weights with Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.wf)\n",
    "        nn.init.xavier_uniform_(self.wi)\n",
    "        nn.init.xavier_uniform_(self.wc)\n",
    "        nn.init.xavier_uniform_(self.wo)\n",
    "        nn.init.xavier_uniform_(self.wy)\n",
    "\n",
    "        # Initialize biases to zeros\n",
    "        nn.init.constant_(self.bf, 0)\n",
    "        nn.init.constant_(self.bi, 0)\n",
    "        nn.init.constant_(self.bc, 0)\n",
    "        nn.init.constant_(self.bo, 0)\n",
    "        nn.init.constant_(self.by, 0)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        outputs = []\n",
    "        seq_length = X.size(0)\n",
    "        hidden_state = torch.zeros(self.hidden_size,1,dtype=torch.float32)\n",
    "        cell_state = torch.zeros(self.hidden_size,1,dtype = torch.float32)\n",
    "\n",
    "        for q in range(seq_length):\n",
    "            concat_input = torch.cat((hidden_state, X[q].unsqueeze(1)), dim=0)\n",
    "            forget_gate = torch.sigmoid(torch.matmul(self.wf, concat_input) + self.bf)\n",
    "            input_gate = torch.sigmoid(torch.matmul(self.wi, concat_input) + self.bi)\n",
    "            candidate_gate = torch.tanh(torch.matmul(self.wc, concat_input) + self.bc)\n",
    "            output_gate = torch.sigmoid(torch.matmul(self.wo, concat_input) + self.bo)\n",
    "\n",
    "            cell_state = forget_gate * cell_state + input_gate * candidate_gate\n",
    "            hidden_state = output_gate * torch.tanh(cell_state)\n",
    "\n",
    "            output = torch.matmul(self.wy, hidden_state) + self.by\n",
    "            outputs.append(output)\n",
    "        outputs = torch.stack(outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        out = [self.forward(x) for x in X]\n",
    "        return torch.stack(out)\n",
    "    \n",
    "    def train(self, X, y, epochs, lr, model_path):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            prediction = self.predict(X)\n",
    "            prediction = prediction.reshape(-1, self.output_size)\n",
    "            target = y.reshape(-1)\n",
    "            loss = criterion(prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "            current_loss = epoch_loss / len(X)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {current_loss:.4f}\")\n",
    "\n",
    "            if (epoch % 100 == 0):\n",
    "                torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': self.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'loss': loss,\n",
    "                            }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 51\n",
    "hidden_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(chars)\n",
    "\n",
    "model = LSTM(hidden_size+input_size, hidden_size, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 0.0008\n",
      "Epoch 2/500, Loss: 0.0008\n",
      "Epoch 3/500, Loss: 0.0008\n",
      "Epoch 4/500, Loss: 0.0008\n",
      "Epoch 5/500, Loss: 0.0008\n",
      "Epoch 6/500, Loss: 0.0007\n",
      "Epoch 7/500, Loss: 0.0007\n",
      "Epoch 8/500, Loss: 0.0007\n",
      "Epoch 9/500, Loss: 0.0007\n",
      "Epoch 10/500, Loss: 0.0007\n",
      "Epoch 11/500, Loss: 0.0007\n",
      "Epoch 12/500, Loss: 0.0006\n",
      "Epoch 13/500, Loss: 0.0006\n",
      "Epoch 14/500, Loss: 0.0006\n",
      "Epoch 15/500, Loss: 0.0006\n",
      "Epoch 16/500, Loss: 0.0006\n",
      "Epoch 17/500, Loss: 0.0006\n",
      "Epoch 18/500, Loss: 0.0006\n",
      "Epoch 19/500, Loss: 0.0006\n",
      "Epoch 20/500, Loss: 0.0006\n",
      "Epoch 21/500, Loss: 0.0006\n",
      "Epoch 22/500, Loss: 0.0006\n",
      "Epoch 23/500, Loss: 0.0006\n",
      "Epoch 24/500, Loss: 0.0006\n",
      "Epoch 25/500, Loss: 0.0006\n",
      "Epoch 26/500, Loss: 0.0006\n",
      "Epoch 27/500, Loss: 0.0006\n",
      "Epoch 28/500, Loss: 0.0006\n",
      "Epoch 29/500, Loss: 0.0006\n",
      "Epoch 30/500, Loss: 0.0006\n",
      "Epoch 31/500, Loss: 0.0006\n",
      "Epoch 32/500, Loss: 0.0006\n",
      "Epoch 33/500, Loss: 0.0006\n",
      "Epoch 34/500, Loss: 0.0006\n",
      "Epoch 35/500, Loss: 0.0006\n",
      "Epoch 36/500, Loss: 0.0006\n",
      "Epoch 37/500, Loss: 0.0005\n",
      "Epoch 38/500, Loss: 0.0005\n",
      "Epoch 39/500, Loss: 0.0005\n",
      "Epoch 40/500, Loss: 0.0005\n",
      "Epoch 41/500, Loss: 0.0005\n",
      "Epoch 42/500, Loss: 0.0005\n",
      "Epoch 43/500, Loss: 0.0005\n",
      "Epoch 44/500, Loss: 0.0005\n",
      "Epoch 45/500, Loss: 0.0005\n",
      "Epoch 46/500, Loss: 0.0005\n",
      "Epoch 47/500, Loss: 0.0005\n",
      "Epoch 48/500, Loss: 0.0005\n",
      "Epoch 49/500, Loss: 0.0005\n",
      "Epoch 50/500, Loss: 0.0005\n",
      "Epoch 51/500, Loss: 0.0005\n",
      "Epoch 52/500, Loss: 0.0005\n",
      "Epoch 53/500, Loss: 0.0005\n",
      "Epoch 54/500, Loss: 0.0005\n",
      "Epoch 55/500, Loss: 0.0005\n",
      "Epoch 56/500, Loss: 0.0005\n",
      "Epoch 57/500, Loss: 0.0005\n",
      "Epoch 58/500, Loss: 0.0005\n",
      "Epoch 59/500, Loss: 0.0005\n",
      "Epoch 60/500, Loss: 0.0005\n",
      "Epoch 61/500, Loss: 0.0005\n",
      "Epoch 62/500, Loss: 0.0004\n",
      "Epoch 63/500, Loss: 0.0004\n",
      "Epoch 64/500, Loss: 0.0004\n",
      "Epoch 65/500, Loss: 0.0004\n",
      "Epoch 66/500, Loss: 0.0004\n",
      "Epoch 67/500, Loss: 0.0004\n",
      "Epoch 68/500, Loss: 0.0004\n",
      "Epoch 69/500, Loss: 0.0004\n",
      "Epoch 70/500, Loss: 0.0004\n",
      "Epoch 71/500, Loss: 0.0004\n",
      "Epoch 72/500, Loss: 0.0004\n",
      "Epoch 73/500, Loss: 0.0004\n",
      "Epoch 74/500, Loss: 0.0004\n",
      "Epoch 75/500, Loss: 0.0004\n",
      "Epoch 76/500, Loss: 0.0004\n",
      "Epoch 77/500, Loss: 0.0004\n",
      "Epoch 78/500, Loss: 0.0004\n",
      "Epoch 79/500, Loss: 0.0004\n",
      "Epoch 80/500, Loss: 0.0004\n",
      "Epoch 81/500, Loss: 0.0004\n",
      "Epoch 82/500, Loss: 0.0004\n",
      "Epoch 83/500, Loss: 0.0004\n",
      "Epoch 84/500, Loss: 0.0004\n",
      "Epoch 85/500, Loss: 0.0004\n",
      "Epoch 86/500, Loss: 0.0004\n",
      "Epoch 87/500, Loss: 0.0004\n",
      "Epoch 88/500, Loss: 0.0004\n",
      "Epoch 89/500, Loss: 0.0004\n",
      "Epoch 90/500, Loss: 0.0004\n",
      "Epoch 91/500, Loss: 0.0004\n",
      "Epoch 92/500, Loss: 0.0004\n",
      "Epoch 93/500, Loss: 0.0004\n",
      "Epoch 94/500, Loss: 0.0004\n",
      "Epoch 95/500, Loss: 0.0004\n",
      "Epoch 96/500, Loss: 0.0004\n",
      "Epoch 97/500, Loss: 0.0004\n",
      "Epoch 98/500, Loss: 0.0004\n",
      "Epoch 99/500, Loss: 0.0004\n",
      "Epoch 100/500, Loss: 0.0004\n",
      "Epoch 101/500, Loss: 0.0004\n",
      "Epoch 102/500, Loss: 0.0004\n",
      "Epoch 103/500, Loss: 0.0004\n",
      "Epoch 104/500, Loss: 0.0004\n",
      "Epoch 105/500, Loss: 0.0004\n",
      "Epoch 106/500, Loss: 0.0004\n",
      "Epoch 107/500, Loss: 0.0003\n",
      "Epoch 108/500, Loss: 0.0003\n",
      "Epoch 109/500, Loss: 0.0003\n",
      "Epoch 110/500, Loss: 0.0003\n",
      "Epoch 111/500, Loss: 0.0003\n",
      "Epoch 112/500, Loss: 0.0003\n",
      "Epoch 113/500, Loss: 0.0003\n",
      "Epoch 114/500, Loss: 0.0003\n",
      "Epoch 115/500, Loss: 0.0003\n",
      "Epoch 116/500, Loss: 0.0003\n",
      "Epoch 117/500, Loss: 0.0003\n",
      "Epoch 118/500, Loss: 0.0003\n",
      "Epoch 119/500, Loss: 0.0003\n",
      "Epoch 120/500, Loss: 0.0003\n",
      "Epoch 121/500, Loss: 0.0003\n",
      "Epoch 122/500, Loss: 0.0003\n",
      "Epoch 123/500, Loss: 0.0003\n",
      "Epoch 124/500, Loss: 0.0003\n",
      "Epoch 125/500, Loss: 0.0003\n",
      "Epoch 126/500, Loss: 0.0003\n",
      "Epoch 127/500, Loss: 0.0003\n",
      "Epoch 128/500, Loss: 0.0003\n",
      "Epoch 129/500, Loss: 0.0003\n",
      "Epoch 130/500, Loss: 0.0003\n",
      "Epoch 131/500, Loss: 0.0003\n",
      "Epoch 132/500, Loss: 0.0003\n",
      "Epoch 133/500, Loss: 0.0003\n",
      "Epoch 134/500, Loss: 0.0003\n",
      "Epoch 135/500, Loss: 0.0003\n",
      "Epoch 136/500, Loss: 0.0003\n",
      "Epoch 137/500, Loss: 0.0003\n",
      "Epoch 138/500, Loss: 0.0003\n",
      "Epoch 139/500, Loss: 0.0003\n",
      "Epoch 140/500, Loss: 0.0003\n",
      "Epoch 141/500, Loss: 0.0003\n",
      "Epoch 142/500, Loss: 0.0003\n",
      "Epoch 143/500, Loss: 0.0003\n",
      "Epoch 144/500, Loss: 0.0003\n",
      "Epoch 145/500, Loss: 0.0003\n",
      "Epoch 146/500, Loss: 0.0003\n",
      "Epoch 147/500, Loss: 0.0003\n",
      "Epoch 148/500, Loss: 0.0003\n",
      "Epoch 149/500, Loss: 0.0003\n",
      "Epoch 150/500, Loss: 0.0003\n",
      "Epoch 151/500, Loss: 0.0003\n",
      "Epoch 152/500, Loss: 0.0003\n",
      "Epoch 153/500, Loss: 0.0003\n",
      "Epoch 154/500, Loss: 0.0003\n",
      "Epoch 155/500, Loss: 0.0003\n",
      "Epoch 156/500, Loss: 0.0003\n",
      "Epoch 157/500, Loss: 0.0003\n",
      "Epoch 158/500, Loss: 0.0003\n",
      "Epoch 159/500, Loss: 0.0003\n",
      "Epoch 160/500, Loss: 0.0003\n",
      "Epoch 161/500, Loss: 0.0003\n",
      "Epoch 162/500, Loss: 0.0003\n",
      "Epoch 163/500, Loss: 0.0003\n",
      "Epoch 164/500, Loss: 0.0003\n",
      "Epoch 165/500, Loss: 0.0003\n",
      "Epoch 166/500, Loss: 0.0003\n",
      "Epoch 167/500, Loss: 0.0003\n",
      "Epoch 168/500, Loss: 0.0003\n",
      "Epoch 169/500, Loss: 0.0003\n",
      "Epoch 170/500, Loss: 0.0003\n",
      "Epoch 171/500, Loss: 0.0003\n",
      "Epoch 172/500, Loss: 0.0003\n",
      "Epoch 173/500, Loss: 0.0003\n",
      "Epoch 174/500, Loss: 0.0003\n",
      "Epoch 175/500, Loss: 0.0003\n",
      "Epoch 176/500, Loss: 0.0003\n",
      "Epoch 177/500, Loss: 0.0003\n",
      "Epoch 178/500, Loss: 0.0003\n",
      "Epoch 179/500, Loss: 0.0003\n",
      "Epoch 180/500, Loss: 0.0003\n",
      "Epoch 181/500, Loss: 0.0003\n",
      "Epoch 182/500, Loss: 0.0003\n",
      "Epoch 183/500, Loss: 0.0003\n",
      "Epoch 184/500, Loss: 0.0003\n",
      "Epoch 185/500, Loss: 0.0002\n",
      "Epoch 186/500, Loss: 0.0002\n",
      "Epoch 187/500, Loss: 0.0002\n",
      "Epoch 188/500, Loss: 0.0002\n",
      "Epoch 189/500, Loss: 0.0002\n",
      "Epoch 190/500, Loss: 0.0002\n",
      "Epoch 191/500, Loss: 0.0002\n",
      "Epoch 192/500, Loss: 0.0002\n",
      "Epoch 193/500, Loss: 0.0002\n",
      "Epoch 194/500, Loss: 0.0002\n",
      "Epoch 195/500, Loss: 0.0002\n",
      "Epoch 196/500, Loss: 0.0002\n",
      "Epoch 197/500, Loss: 0.0002\n",
      "Epoch 198/500, Loss: 0.0002\n",
      "Epoch 199/500, Loss: 0.0002\n",
      "Epoch 200/500, Loss: 0.0002\n",
      "Epoch 201/500, Loss: 0.0002\n",
      "Epoch 202/500, Loss: 0.0002\n",
      "Epoch 203/500, Loss: 0.0002\n",
      "Epoch 204/500, Loss: 0.0002\n",
      "Epoch 205/500, Loss: 0.0002\n",
      "Epoch 206/500, Loss: 0.0002\n",
      "Epoch 207/500, Loss: 0.0002\n",
      "Epoch 208/500, Loss: 0.0002\n",
      "Epoch 209/500, Loss: 0.0002\n",
      "Epoch 210/500, Loss: 0.0002\n",
      "Epoch 211/500, Loss: 0.0002\n",
      "Epoch 212/500, Loss: 0.0002\n",
      "Epoch 213/500, Loss: 0.0002\n",
      "Epoch 214/500, Loss: 0.0002\n",
      "Epoch 215/500, Loss: 0.0002\n",
      "Epoch 216/500, Loss: 0.0002\n",
      "Epoch 217/500, Loss: 0.0002\n",
      "Epoch 218/500, Loss: 0.0002\n",
      "Epoch 219/500, Loss: 0.0002\n",
      "Epoch 220/500, Loss: 0.0002\n",
      "Epoch 221/500, Loss: 0.0002\n",
      "Epoch 222/500, Loss: 0.0002\n",
      "Epoch 223/500, Loss: 0.0002\n",
      "Epoch 224/500, Loss: 0.0002\n",
      "Epoch 225/500, Loss: 0.0002\n",
      "Epoch 226/500, Loss: 0.0002\n",
      "Epoch 227/500, Loss: 0.0002\n",
      "Epoch 228/500, Loss: 0.0002\n",
      "Epoch 229/500, Loss: 0.0002\n",
      "Epoch 230/500, Loss: 0.0002\n",
      "Epoch 231/500, Loss: 0.0002\n",
      "Epoch 232/500, Loss: 0.0002\n",
      "Epoch 233/500, Loss: 0.0002\n",
      "Epoch 234/500, Loss: 0.0002\n",
      "Epoch 235/500, Loss: 0.0002\n",
      "Epoch 236/500, Loss: 0.0002\n",
      "Epoch 237/500, Loss: 0.0002\n",
      "Epoch 238/500, Loss: 0.0002\n",
      "Epoch 239/500, Loss: 0.0002\n",
      "Epoch 240/500, Loss: 0.0002\n",
      "Epoch 241/500, Loss: 0.0002\n",
      "Epoch 242/500, Loss: 0.0002\n",
      "Epoch 243/500, Loss: 0.0002\n",
      "Epoch 244/500, Loss: 0.0002\n",
      "Epoch 245/500, Loss: 0.0002\n",
      "Epoch 246/500, Loss: 0.0002\n",
      "Epoch 247/500, Loss: 0.0002\n",
      "Epoch 248/500, Loss: 0.0002\n",
      "Epoch 249/500, Loss: 0.0002\n",
      "Epoch 250/500, Loss: 0.0002\n",
      "Epoch 251/500, Loss: 0.0002\n",
      "Epoch 252/500, Loss: 0.0002\n",
      "Epoch 253/500, Loss: 0.0002\n",
      "Epoch 254/500, Loss: 0.0002\n",
      "Epoch 255/500, Loss: 0.0002\n",
      "Epoch 256/500, Loss: 0.0002\n",
      "Epoch 257/500, Loss: 0.0002\n",
      "Epoch 258/500, Loss: 0.0002\n",
      "Epoch 259/500, Loss: 0.0002\n",
      "Epoch 260/500, Loss: 0.0002\n",
      "Epoch 261/500, Loss: 0.0002\n",
      "Epoch 262/500, Loss: 0.0002\n",
      "Epoch 263/500, Loss: 0.0002\n",
      "Epoch 264/500, Loss: 0.0002\n",
      "Epoch 265/500, Loss: 0.0002\n",
      "Epoch 266/500, Loss: 0.0002\n",
      "Epoch 267/500, Loss: 0.0002\n",
      "Epoch 268/500, Loss: 0.0002\n",
      "Epoch 269/500, Loss: 0.0002\n",
      "Epoch 270/500, Loss: 0.0002\n",
      "Epoch 271/500, Loss: 0.0002\n",
      "Epoch 272/500, Loss: 0.0002\n",
      "Epoch 273/500, Loss: 0.0002\n",
      "Epoch 274/500, Loss: 0.0002\n",
      "Epoch 275/500, Loss: 0.0002\n",
      "Epoch 276/500, Loss: 0.0002\n",
      "Epoch 277/500, Loss: 0.0002\n",
      "Epoch 278/500, Loss: 0.0002\n",
      "Epoch 279/500, Loss: 0.0002\n",
      "Epoch 280/500, Loss: 0.0002\n",
      "Epoch 281/500, Loss: 0.0002\n",
      "Epoch 282/500, Loss: 0.0002\n",
      "Epoch 283/500, Loss: 0.0002\n",
      "Epoch 284/500, Loss: 0.0002\n",
      "Epoch 285/500, Loss: 0.0002\n",
      "Epoch 286/500, Loss: 0.0002\n",
      "Epoch 287/500, Loss: 0.0002\n",
      "Epoch 288/500, Loss: 0.0002\n",
      "Epoch 289/500, Loss: 0.0002\n",
      "Epoch 290/500, Loss: 0.0002\n",
      "Epoch 291/500, Loss: 0.0002\n",
      "Epoch 292/500, Loss: 0.0002\n",
      "Epoch 293/500, Loss: 0.0002\n",
      "Epoch 294/500, Loss: 0.0002\n",
      "Epoch 295/500, Loss: 0.0002\n",
      "Epoch 296/500, Loss: 0.0002\n",
      "Epoch 297/500, Loss: 0.0002\n",
      "Epoch 298/500, Loss: 0.0002\n",
      "Epoch 299/500, Loss: 0.0002\n",
      "Epoch 300/500, Loss: 0.0002\n",
      "Epoch 301/500, Loss: 0.0002\n",
      "Epoch 302/500, Loss: 0.0002\n",
      "Epoch 303/500, Loss: 0.0002\n",
      "Epoch 304/500, Loss: 0.0002\n",
      "Epoch 305/500, Loss: 0.0002\n",
      "Epoch 306/500, Loss: 0.0002\n",
      "Epoch 307/500, Loss: 0.0002\n",
      "Epoch 308/500, Loss: 0.0002\n",
      "Epoch 309/500, Loss: 0.0002\n",
      "Epoch 310/500, Loss: 0.0002\n",
      "Epoch 311/500, Loss: 0.0002\n",
      "Epoch 312/500, Loss: 0.0002\n",
      "Epoch 313/500, Loss: 0.0002\n",
      "Epoch 314/500, Loss: 0.0002\n",
      "Epoch 315/500, Loss: 0.0002\n",
      "Epoch 316/500, Loss: 0.0002\n",
      "Epoch 317/500, Loss: 0.0002\n",
      "Epoch 318/500, Loss: 0.0002\n",
      "Epoch 319/500, Loss: 0.0002\n",
      "Epoch 320/500, Loss: 0.0002\n",
      "Epoch 321/500, Loss: 0.0002\n",
      "Epoch 322/500, Loss: 0.0002\n",
      "Epoch 323/500, Loss: 0.0002\n",
      "Epoch 324/500, Loss: 0.0002\n",
      "Epoch 325/500, Loss: 0.0002\n",
      "Epoch 326/500, Loss: 0.0002\n",
      "Epoch 327/500, Loss: 0.0002\n",
      "Epoch 328/500, Loss: 0.0002\n",
      "Epoch 329/500, Loss: 0.0002\n",
      "Epoch 330/500, Loss: 0.0002\n",
      "Epoch 331/500, Loss: 0.0002\n",
      "Epoch 332/500, Loss: 0.0002\n",
      "Epoch 333/500, Loss: 0.0002\n",
      "Epoch 334/500, Loss: 0.0002\n",
      "Epoch 335/500, Loss: 0.0002\n",
      "Epoch 336/500, Loss: 0.0002\n",
      "Epoch 337/500, Loss: 0.0002\n",
      "Epoch 338/500, Loss: 0.0002\n",
      "Epoch 339/500, Loss: 0.0002\n",
      "Epoch 340/500, Loss: 0.0002\n",
      "Epoch 341/500, Loss: 0.0002\n",
      "Epoch 342/500, Loss: 0.0002\n",
      "Epoch 343/500, Loss: 0.0002\n",
      "Epoch 344/500, Loss: 0.0002\n",
      "Epoch 345/500, Loss: 0.0002\n",
      "Epoch 346/500, Loss: 0.0002\n",
      "Epoch 347/500, Loss: 0.0002\n",
      "Epoch 348/500, Loss: 0.0002\n",
      "Epoch 349/500, Loss: 0.0002\n",
      "Epoch 350/500, Loss: 0.0002\n",
      "Epoch 351/500, Loss: 0.0002\n",
      "Epoch 352/500, Loss: 0.0002\n",
      "Epoch 353/500, Loss: 0.0002\n",
      "Epoch 354/500, Loss: 0.0002\n",
      "Epoch 355/500, Loss: 0.0002\n",
      "Epoch 356/500, Loss: 0.0002\n",
      "Epoch 357/500, Loss: 0.0002\n",
      "Epoch 358/500, Loss: 0.0002\n",
      "Epoch 359/500, Loss: 0.0002\n",
      "Epoch 360/500, Loss: 0.0002\n",
      "Epoch 361/500, Loss: 0.0002\n",
      "Epoch 362/500, Loss: 0.0002\n",
      "Epoch 363/500, Loss: 0.0002\n",
      "Epoch 364/500, Loss: 0.0002\n",
      "Epoch 365/500, Loss: 0.0002\n",
      "Epoch 366/500, Loss: 0.0002\n",
      "Epoch 367/500, Loss: 0.0002\n",
      "Epoch 368/500, Loss: 0.0002\n",
      "Epoch 369/500, Loss: 0.0002\n",
      "Epoch 370/500, Loss: 0.0002\n",
      "Epoch 371/500, Loss: 0.0002\n",
      "Epoch 372/500, Loss: 0.0002\n",
      "Epoch 373/500, Loss: 0.0002\n",
      "Epoch 374/500, Loss: 0.0002\n",
      "Epoch 375/500, Loss: 0.0002\n",
      "Epoch 376/500, Loss: 0.0002\n",
      "Epoch 377/500, Loss: 0.0002\n",
      "Epoch 378/500, Loss: 0.0002\n",
      "Epoch 379/500, Loss: 0.0002\n",
      "Epoch 380/500, Loss: 0.0002\n",
      "Epoch 381/500, Loss: 0.0002\n",
      "Epoch 382/500, Loss: 0.0002\n",
      "Epoch 383/500, Loss: 0.0002\n",
      "Epoch 384/500, Loss: 0.0002\n",
      "Epoch 385/500, Loss: 0.0002\n",
      "Epoch 386/500, Loss: 0.0002\n",
      "Epoch 387/500, Loss: 0.0002\n",
      "Epoch 388/500, Loss: 0.0002\n",
      "Epoch 389/500, Loss: 0.0002\n",
      "Epoch 390/500, Loss: 0.0002\n",
      "Epoch 391/500, Loss: 0.0002\n",
      "Epoch 392/500, Loss: 0.0002\n",
      "Epoch 393/500, Loss: 0.0002\n",
      "Epoch 394/500, Loss: 0.0002\n",
      "Epoch 395/500, Loss: 0.0002\n",
      "Epoch 396/500, Loss: 0.0001\n",
      "Epoch 397/500, Loss: 0.0001\n",
      "Epoch 398/500, Loss: 0.0001\n",
      "Epoch 399/500, Loss: 0.0001\n",
      "Epoch 400/500, Loss: 0.0001\n",
      "Epoch 401/500, Loss: 0.0001\n",
      "Epoch 402/500, Loss: 0.0001\n",
      "Epoch 403/500, Loss: 0.0001\n",
      "Epoch 404/500, Loss: 0.0001\n",
      "Epoch 405/500, Loss: 0.0001\n",
      "Epoch 406/500, Loss: 0.0001\n",
      "Epoch 407/500, Loss: 0.0001\n",
      "Epoch 408/500, Loss: 0.0001\n",
      "Epoch 409/500, Loss: 0.0001\n",
      "Epoch 410/500, Loss: 0.0001\n",
      "Epoch 411/500, Loss: 0.0001\n",
      "Epoch 412/500, Loss: 0.0001\n",
      "Epoch 413/500, Loss: 0.0001\n",
      "Epoch 414/500, Loss: 0.0001\n",
      "Epoch 415/500, Loss: 0.0001\n",
      "Epoch 416/500, Loss: 0.0001\n",
      "Epoch 417/500, Loss: 0.0001\n",
      "Epoch 418/500, Loss: 0.0001\n",
      "Epoch 419/500, Loss: 0.0001\n",
      "Epoch 420/500, Loss: 0.0001\n",
      "Epoch 421/500, Loss: 0.0001\n",
      "Epoch 422/500, Loss: 0.0001\n",
      "Epoch 423/500, Loss: 0.0001\n",
      "Epoch 424/500, Loss: 0.0001\n",
      "Epoch 425/500, Loss: 0.0001\n",
      "Epoch 426/500, Loss: 0.0001\n",
      "Epoch 427/500, Loss: 0.0001\n",
      "Epoch 428/500, Loss: 0.0001\n",
      "Epoch 429/500, Loss: 0.0001\n",
      "Epoch 430/500, Loss: 0.0001\n",
      "Epoch 431/500, Loss: 0.0001\n",
      "Epoch 432/500, Loss: 0.0001\n",
      "Epoch 433/500, Loss: 0.0001\n",
      "Epoch 434/500, Loss: 0.0001\n",
      "Epoch 435/500, Loss: 0.0001\n",
      "Epoch 436/500, Loss: 0.0001\n",
      "Epoch 437/500, Loss: 0.0001\n",
      "Epoch 438/500, Loss: 0.0001\n",
      "Epoch 439/500, Loss: 0.0001\n",
      "Epoch 440/500, Loss: 0.0001\n",
      "Epoch 441/500, Loss: 0.0001\n",
      "Epoch 442/500, Loss: 0.0001\n",
      "Epoch 443/500, Loss: 0.0001\n",
      "Epoch 444/500, Loss: 0.0001\n",
      "Epoch 445/500, Loss: 0.0001\n",
      "Epoch 446/500, Loss: 0.0001\n",
      "Epoch 447/500, Loss: 0.0001\n",
      "Epoch 448/500, Loss: 0.0001\n",
      "Epoch 449/500, Loss: 0.0001\n",
      "Epoch 450/500, Loss: 0.0001\n",
      "Epoch 451/500, Loss: 0.0001\n",
      "Epoch 452/500, Loss: 0.0001\n",
      "Epoch 453/500, Loss: 0.0001\n",
      "Epoch 454/500, Loss: 0.0001\n",
      "Epoch 455/500, Loss: 0.0001\n",
      "Epoch 456/500, Loss: 0.0001\n",
      "Epoch 457/500, Loss: 0.0001\n",
      "Epoch 458/500, Loss: 0.0001\n",
      "Epoch 459/500, Loss: 0.0001\n",
      "Epoch 460/500, Loss: 0.0001\n",
      "Epoch 461/500, Loss: 0.0001\n",
      "Epoch 462/500, Loss: 0.0001\n",
      "Epoch 463/500, Loss: 0.0001\n",
      "Epoch 464/500, Loss: 0.0001\n",
      "Epoch 465/500, Loss: 0.0001\n",
      "Epoch 466/500, Loss: 0.0001\n",
      "Epoch 467/500, Loss: 0.0001\n",
      "Epoch 468/500, Loss: 0.0001\n",
      "Epoch 469/500, Loss: 0.0001\n",
      "Epoch 470/500, Loss: 0.0001\n",
      "Epoch 471/500, Loss: 0.0001\n",
      "Epoch 472/500, Loss: 0.0001\n",
      "Epoch 473/500, Loss: 0.0001\n",
      "Epoch 474/500, Loss: 0.0001\n",
      "Epoch 475/500, Loss: 0.0001\n",
      "Epoch 476/500, Loss: 0.0001\n",
      "Epoch 477/500, Loss: 0.0001\n",
      "Epoch 478/500, Loss: 0.0001\n",
      "Epoch 479/500, Loss: 0.0001\n",
      "Epoch 480/500, Loss: 0.0001\n",
      "Epoch 481/500, Loss: 0.0001\n",
      "Epoch 482/500, Loss: 0.0001\n",
      "Epoch 483/500, Loss: 0.0001\n",
      "Epoch 484/500, Loss: 0.0001\n",
      "Epoch 485/500, Loss: 0.0001\n",
      "Epoch 486/500, Loss: 0.0001\n",
      "Epoch 487/500, Loss: 0.0001\n",
      "Epoch 488/500, Loss: 0.0001\n",
      "Epoch 489/500, Loss: 0.0001\n",
      "Epoch 490/500, Loss: 0.0001\n",
      "Epoch 491/500, Loss: 0.0001\n",
      "Epoch 492/500, Loss: 0.0001\n",
      "Epoch 493/500, Loss: 0.0001\n",
      "Epoch 494/500, Loss: 0.0001\n",
      "Epoch 495/500, Loss: 0.0001\n",
      "Epoch 496/500, Loss: 0.0001\n",
      "Epoch 497/500, Loss: 0.0001\n",
      "Epoch 498/500, Loss: 0.0001\n",
      "Epoch 499/500, Loss: 0.0001\n",
      "Epoch 500/500, Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "checkpoint_folder = \"/Users/nimitt/Documents/DigitalSystems/Project/Model_States\"\n",
    "model_filename = f\"model_{block_size}_{hidden_size}_{text_len}.pt\"\n",
    "model_path = os.path.join(checkpoint_folder, model_filename)\n",
    "# Ensure the folder exists\n",
    "os.makedirs(checkpoint_folder, exist_ok=True)\n",
    "\n",
    "model.train(X,Y,500,0.01,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "def convert_to_X(prompt):\n",
    "    X_ = np.zeros((len(prompt),input_size))\n",
    "    for i in range(len(prompt)):\n",
    "        X_[i][stoi[prompt[i]]] = 1\n",
    "    return torch.tensor(X_,dtype = torch.float32)\n",
    "        \n",
    "prompt = \"alumni\"\n",
    "max_len = 100\n",
    "context = []\n",
    "for j in range(len(prompt)):\n",
    "    context = context + [stoi[prompt[j]]]\n",
    "context = context[-block_size:]\n",
    "\n",
    "generated_text = prompt\n",
    "for i in range(max_len):\n",
    "    x = convert_to_X(generated_text)\n",
    "    y_pred = model(x)[-1]\n",
    "    # ix = torch.distributions.categorical.Categorical(logits=y_pred.squeeze()).sample()\n",
    "    ix = torch.argmax(y_pred)\n",
    "    ch = itos[ix.item()]\n",
    "    generated_text += ch\n",
    "    context = context[1:] + [ix]\n",
    "\n",
    "genrated_text = generated_text.replace('|','\\n')\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alumni relations advisory 02 (version 1.0, december 2021)                                 page 1 of 1 ~ indian institute of technology gandhinagar  ~ ~honorary  alumni program at iit gandhinagar  ~(as approved by the bog in its 33rd meeting held on 15 november 2021)  ~ ~ ~the board of governors of the institute in its 33rd meeting held on 15 november ~2021 approved the following honorary alumni program  at iit gandhinagar.  ~  ~1. individuals who are not graduates of iit gandhinagar and who make'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| alumni relations and operal safety & health legislations  ~cremics |\n"
     ]
    }
   ],
   "source": [
    "model_read = LSTM(input_size+hidden_size, hidden_size, input_size)\n",
    "model_path = \"/Users/nimitt/Documents/DigitalSystems/Project/Model_States/model_10_50_5000.pt\"\n",
    "checkpoint = torch.load(model_path,map_location=torch.device('cpu'))\n",
    "model_read.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Testing\n",
    "\n",
    "def convert_to_X(prompt):\n",
    "    X_ = np.zeros((len(prompt),input_size))\n",
    "    for i in range(len(prompt)):\n",
    "        X_[i][stoi[prompt[i]]] = 1\n",
    "    return torch.tensor(X_,dtype = torch.float32)\n",
    "        \n",
    "prompt = \"alumni relations\"\n",
    "max_len = 50\n",
    "context = []\n",
    "for j in range(len(prompt)):\n",
    "    context = context + [stoi[prompt[j]]]\n",
    "context = context[-block_size:]\n",
    "\n",
    "generated_text = prompt\n",
    "for i in range(max_len):\n",
    "    x = convert_to_X(generated_text)\n",
    "    y_pred = model_read(x)[-1]\n",
    "    # ix = torch.distributions.categorical.Categorical(logits=y_pred.squeeze()).sample()\n",
    "    ix = torch.argmax(y_pred)\n",
    "    ch = itos[ix.item()]\n",
    "    generated_text += ch\n",
    "    context = context[1:] + [ix]\n",
    "\n",
    "genrated_text = generated_text.replace('|','\\n')\n",
    "print(\"|\",generated_text,\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
