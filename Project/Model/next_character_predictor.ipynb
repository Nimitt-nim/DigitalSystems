{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "with open('/Users/nimitt/Documents/DigitalSystems/Project/Datasets/advisories.txt','r') as file:\n",
    "    text = file.read()\n",
    "text = text.lower()\n",
    "\n",
    "chars = ['\\x00',\n",
    " '\\t',\n",
    " '\\n',\n",
    " ' ',\n",
    " '!',\n",
    " '\"',\n",
    " '#',\n",
    " '$',\n",
    " '%',\n",
    " '&',\n",
    " \"'\",\n",
    " '(',\n",
    " ')',\n",
    " '*',\n",
    " '+',\n",
    " ',',\n",
    " '-',\n",
    " '.',\n",
    " '/',\n",
    " '0',\n",
    " '1',\n",
    " '2',\n",
    " '3',\n",
    " '4',\n",
    " '5',\n",
    " '6',\n",
    " '7',\n",
    " '8',\n",
    " '9',\n",
    " ':',\n",
    " ';',\n",
    " '<',\n",
    " '=',\n",
    " '>',\n",
    " '?',\n",
    " '@',\n",
    " '[',\n",
    " ']',\n",
    " '^',\n",
    " '_',\n",
    " '`',\n",
    " 'a',\n",
    " 'b',\n",
    " 'c',\n",
    " 'd',\n",
    " 'e',\n",
    " 'f',\n",
    " 'g',\n",
    " 'h',\n",
    " 'i',\n",
    " 'j',\n",
    " 'k',\n",
    " 'l',\n",
    " 'm',\n",
    " 'n',\n",
    " 'o',\n",
    " 'p',\n",
    " 'q',\n",
    " 'r',\n",
    " 's',\n",
    " 't',\n",
    " 'u',\n",
    " 'v',\n",
    " 'w',\n",
    " 'x',\n",
    " 'y',\n",
    " 'z',\n",
    " '~',\n",
    " '\\xa0',\n",
    " '¬ß',\n",
    " '¬∑',\n",
    " '‡§Ç',\n",
    " '‡§ï',\n",
    " '‡§ó',\n",
    " '‡§§',\n",
    " '‡§•',\n",
    " '‡§¶',\n",
    " '‡§ß',\n",
    " '‡§®',\n",
    " '‡§™',\n",
    " '‡§≠',\n",
    " '‡§Ø',\n",
    " '‡§∞',\n",
    " '‡§∏',\n",
    " '‡§æ',\n",
    " '‡§ø',\n",
    " '‡•Ä',\n",
    " '‡•ã',\n",
    " '‡•å',\n",
    " '‡•ç',\n",
    " '‚Äì',\n",
    " '‚Äò',\n",
    " '‚Äô',\n",
    " '‚Äú',\n",
    " '‚Äù',\n",
    " '‚Äû',\n",
    " '‚Äü',\n",
    " '‚Ä¢',\n",
    " '‚Ä¶',\n",
    " '\\u2028',\n",
    " '‚Çπ',\n",
    " '‚Üí',\n",
    " '‚àë',\n",
    " '‚àö',\n",
    " '‚â•',\n",
    " '‚ñ°',\n",
    " '‚ñ™',\n",
    " '‚óã',\n",
    " '‚óè',\n",
    " '‚û¢',\n",
    " '\\uf020',\n",
    " '\\uf02d',\n",
    " '\\uf0a7',\n",
    " '\\uf0b7',\n",
    " '\\uf0d8',\n",
    " '\\uf0fc',\n",
    " 'Ô¨Ä',\n",
    " 'Ô¨Å',\n",
    " 'Ô¨É',\n",
    " 'ùê∂',\n",
    " 'ùêº',\n",
    " 'ùëÉ',\n",
    " 'ùëÜ',\n",
    " 'ùëê',\n",
    " 'ùëî']\n",
    "\n",
    "#  Removing unwanted chars\n",
    "unwanted_chars = chars[68:]\n",
    "for unwanted_char in unwanted_chars:\n",
    "    if (unwanted_char in ['‚Ä¢',\n",
    "                            '‚Ä¶',\n",
    "                            '\\u2028',\n",
    "                            '‚Çπ',\n",
    "                            '‚Üí',\n",
    "                            '‚àë','‚àö','‚â•','‚ñ°','‚ñ™','‚óã','‚óè','‚û¢','\\uf020','\\uf02d','\\uf0a7','\\uf0b7','\\uf0d8','\\uf0fc',]):\n",
    "        text = text.replace(unwanted_char,\"|\")\n",
    "    elif (unwanted_char in [ '‚Äò',\n",
    " '‚Äô',\n",
    " '‚Äú',\n",
    " '‚Äù',\n",
    " '‚Äû',\n",
    " '‚Äü',]):\n",
    "        text = text.replace(unwanted_char,\"'\")\n",
    "    else:\n",
    "        text = text.replace(unwanted_char,\"~\")\n",
    "text = text.replace('\\n',\"~\")\n",
    "text = text.replace('\\t',\"~\")\n",
    "text = text.replace('\\x00',\"~\")\n",
    "text_len = 5000\n",
    "text = text[:text_len]\n",
    "# Vocabulary\n",
    "chars = sorted(set(text))   \n",
    "\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# Creating X and Y\n",
    "X, Y = [],[]\n",
    "context = []\n",
    "for j in range(block_size):\n",
    "    context = context + [stoi[text[j]]]\n",
    "\n",
    "for i in range(block_size, len(text)-1):\n",
    "\n",
    "    X_ = np.zeros((block_size,len(chars)))\n",
    "\n",
    "    for j in range(block_size):\n",
    "        X_[j][context[j]] = 1\n",
    "\n",
    "    ch = text[i]\n",
    "    ix = stoi[ch]\n",
    "\n",
    "\n",
    "    X.append(X_)\n",
    "    \n",
    "    context = context[1:] + [ix] \n",
    "    Y.append(context)\n",
    "\n",
    "X = torch.tensor(np.array(X),dtype=torch.float32)\n",
    "Y = torch.tensor(np.array(Y),dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '&': 1,\n",
       " \"'\": 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " ',': 5,\n",
       " '-': 6,\n",
       " '.': 7,\n",
       " '/': 8,\n",
       " '0': 9,\n",
       " '1': 10,\n",
       " '2': 11,\n",
       " '3': 12,\n",
       " '4': 13,\n",
       " '5': 14,\n",
       " '6': 15,\n",
       " '7': 16,\n",
       " '8': 17,\n",
       " '9': 18,\n",
       " ':': 19,\n",
       " ';': 20,\n",
       " '[': 21,\n",
       " ']': 22,\n",
       " 'a': 23,\n",
       " 'b': 24,\n",
       " 'c': 25,\n",
       " 'd': 26,\n",
       " 'e': 27,\n",
       " 'f': 28,\n",
       " 'g': 29,\n",
       " 'h': 30,\n",
       " 'i': 31,\n",
       " 'j': 32,\n",
       " 'k': 33,\n",
       " 'l': 34,\n",
       " 'm': 35,\n",
       " 'n': 36,\n",
       " 'o': 37,\n",
       " 'p': 38,\n",
       " 'q': 39,\n",
       " 'r': 40,\n",
       " 's': 41,\n",
       " 't': 42,\n",
       " 'u': 43,\n",
       " 'v': 44,\n",
       " 'w': 45,\n",
       " 'x': 46,\n",
       " 'y': 47,\n",
       " 'z': 48,\n",
       " '|': 49,\n",
       " '~': 50}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Forget Gate\n",
    "        self.wf = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.bf = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "        # Input Gate\n",
    "        self.wi = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.bi = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "        # Candidate Gate\n",
    "        self.wc = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.bc = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "        # Output Gate\n",
    "        self.wo = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.bo = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "        # Final Gate\n",
    "        self.wy = nn.Parameter(torch.Tensor(output_size, hidden_size))\n",
    "        self.by = nn.Parameter(torch.Tensor(output_size, 1))\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        # Initialize weights with Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.wf)\n",
    "        nn.init.xavier_uniform_(self.wi)\n",
    "        nn.init.xavier_uniform_(self.wc)\n",
    "        nn.init.xavier_uniform_(self.wo)\n",
    "        nn.init.xavier_uniform_(self.wy)\n",
    "\n",
    "        # Initialize biases to zeros\n",
    "        nn.init.constant_(self.bf, 0)\n",
    "        nn.init.constant_(self.bi, 0)\n",
    "        nn.init.constant_(self.bc, 0)\n",
    "        nn.init.constant_(self.bo, 0)\n",
    "        nn.init.constant_(self.by, 0)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        outputs = []\n",
    "        seq_length = X.size(0)\n",
    "        hidden_state = torch.zeros(self.hidden_size,1,dtype=torch.float32)\n",
    "        cell_state = torch.zeros(self.hidden_size,1,dtype = torch.float32)\n",
    "\n",
    "        for q in range(seq_length):\n",
    "            concat_input = torch.cat((hidden_state, X[q].unsqueeze(1)), dim=0)\n",
    "            forget_gate = torch.sigmoid(torch.matmul(self.wf, concat_input) + self.bf)\n",
    "            input_gate = torch.sigmoid(torch.matmul(self.wi, concat_input) + self.bi)\n",
    "            candidate_gate = torch.tanh(torch.matmul(self.wc, concat_input) + self.bc)\n",
    "            output_gate = torch.sigmoid(torch.matmul(self.wo, concat_input) + self.bo)\n",
    "\n",
    "            cell_state = forget_gate * cell_state + input_gate * candidate_gate\n",
    "            hidden_state = output_gate * torch.tanh(cell_state)\n",
    "\n",
    "            output = torch.matmul(self.wy, hidden_state) + self.by\n",
    "            outputs.append(output)\n",
    "            print(candidate_gate)\n",
    "        outputs = torch.stack(outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        out = [self.forward(x) for x in X]\n",
    "        return torch.stack(out)\n",
    "    \n",
    "    def train(self, X, y, epochs, lr, model_path):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            prediction = self.predict(X)\n",
    "            prediction = prediction.reshape(-1, self.output_size)\n",
    "            target = y.reshape(-1)\n",
    "            loss = criterion(prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "            current_loss = epoch_loss / len(X)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {current_loss:.4f}\")\n",
    "\n",
    "            if (epoch % 100 == 0):\n",
    "                torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': self.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'loss': loss,\n",
    "                            }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 50\n",
    "input_size = len(chars)\n",
    "\n",
    "model = LSTM(hidden_size+input_size, hidden_size, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 0.0008\n",
      "Epoch 2/500, Loss: 0.0008\n",
      "Epoch 3/500, Loss: 0.0008\n",
      "Epoch 4/500, Loss: 0.0008\n",
      "Epoch 5/500, Loss: 0.0008\n",
      "Epoch 6/500, Loss: 0.0007\n",
      "Epoch 7/500, Loss: 0.0007\n",
      "Epoch 8/500, Loss: 0.0007\n",
      "Epoch 9/500, Loss: 0.0007\n",
      "Epoch 10/500, Loss: 0.0007\n",
      "Epoch 11/500, Loss: 0.0007\n",
      "Epoch 12/500, Loss: 0.0006\n",
      "Epoch 13/500, Loss: 0.0006\n",
      "Epoch 14/500, Loss: 0.0006\n",
      "Epoch 15/500, Loss: 0.0006\n",
      "Epoch 16/500, Loss: 0.0006\n",
      "Epoch 17/500, Loss: 0.0006\n",
      "Epoch 18/500, Loss: 0.0006\n",
      "Epoch 19/500, Loss: 0.0006\n",
      "Epoch 20/500, Loss: 0.0006\n",
      "Epoch 21/500, Loss: 0.0006\n",
      "Epoch 22/500, Loss: 0.0006\n",
      "Epoch 23/500, Loss: 0.0006\n",
      "Epoch 24/500, Loss: 0.0006\n",
      "Epoch 25/500, Loss: 0.0006\n",
      "Epoch 26/500, Loss: 0.0006\n",
      "Epoch 27/500, Loss: 0.0006\n",
      "Epoch 28/500, Loss: 0.0006\n",
      "Epoch 29/500, Loss: 0.0006\n",
      "Epoch 30/500, Loss: 0.0006\n",
      "Epoch 31/500, Loss: 0.0006\n",
      "Epoch 32/500, Loss: 0.0006\n",
      "Epoch 33/500, Loss: 0.0006\n",
      "Epoch 34/500, Loss: 0.0006\n",
      "Epoch 35/500, Loss: 0.0006\n",
      "Epoch 36/500, Loss: 0.0006\n",
      "Epoch 37/500, Loss: 0.0005\n",
      "Epoch 38/500, Loss: 0.0005\n",
      "Epoch 39/500, Loss: 0.0005\n",
      "Epoch 40/500, Loss: 0.0005\n",
      "Epoch 41/500, Loss: 0.0005\n",
      "Epoch 42/500, Loss: 0.0005\n",
      "Epoch 43/500, Loss: 0.0005\n",
      "Epoch 44/500, Loss: 0.0005\n",
      "Epoch 45/500, Loss: 0.0005\n",
      "Epoch 46/500, Loss: 0.0005\n",
      "Epoch 47/500, Loss: 0.0005\n",
      "Epoch 48/500, Loss: 0.0005\n",
      "Epoch 49/500, Loss: 0.0005\n",
      "Epoch 50/500, Loss: 0.0005\n",
      "Epoch 51/500, Loss: 0.0005\n",
      "Epoch 52/500, Loss: 0.0005\n",
      "Epoch 53/500, Loss: 0.0005\n",
      "Epoch 54/500, Loss: 0.0005\n",
      "Epoch 55/500, Loss: 0.0005\n",
      "Epoch 56/500, Loss: 0.0005\n",
      "Epoch 57/500, Loss: 0.0005\n",
      "Epoch 58/500, Loss: 0.0005\n",
      "Epoch 59/500, Loss: 0.0005\n",
      "Epoch 60/500, Loss: 0.0005\n",
      "Epoch 61/500, Loss: 0.0005\n",
      "Epoch 62/500, Loss: 0.0004\n",
      "Epoch 63/500, Loss: 0.0004\n",
      "Epoch 64/500, Loss: 0.0004\n",
      "Epoch 65/500, Loss: 0.0004\n",
      "Epoch 66/500, Loss: 0.0004\n",
      "Epoch 67/500, Loss: 0.0004\n",
      "Epoch 68/500, Loss: 0.0004\n",
      "Epoch 69/500, Loss: 0.0004\n",
      "Epoch 70/500, Loss: 0.0004\n",
      "Epoch 71/500, Loss: 0.0004\n",
      "Epoch 72/500, Loss: 0.0004\n",
      "Epoch 73/500, Loss: 0.0004\n",
      "Epoch 74/500, Loss: 0.0004\n",
      "Epoch 75/500, Loss: 0.0004\n",
      "Epoch 76/500, Loss: 0.0004\n",
      "Epoch 77/500, Loss: 0.0004\n",
      "Epoch 78/500, Loss: 0.0004\n",
      "Epoch 79/500, Loss: 0.0004\n",
      "Epoch 80/500, Loss: 0.0004\n",
      "Epoch 81/500, Loss: 0.0004\n",
      "Epoch 82/500, Loss: 0.0004\n",
      "Epoch 83/500, Loss: 0.0004\n",
      "Epoch 84/500, Loss: 0.0004\n",
      "Epoch 85/500, Loss: 0.0004\n",
      "Epoch 86/500, Loss: 0.0004\n",
      "Epoch 87/500, Loss: 0.0004\n",
      "Epoch 88/500, Loss: 0.0004\n",
      "Epoch 89/500, Loss: 0.0004\n",
      "Epoch 90/500, Loss: 0.0004\n",
      "Epoch 91/500, Loss: 0.0004\n",
      "Epoch 92/500, Loss: 0.0004\n",
      "Epoch 93/500, Loss: 0.0004\n",
      "Epoch 94/500, Loss: 0.0004\n",
      "Epoch 95/500, Loss: 0.0004\n",
      "Epoch 96/500, Loss: 0.0004\n",
      "Epoch 97/500, Loss: 0.0004\n",
      "Epoch 98/500, Loss: 0.0004\n",
      "Epoch 99/500, Loss: 0.0004\n",
      "Epoch 100/500, Loss: 0.0004\n",
      "Epoch 101/500, Loss: 0.0004\n",
      "Epoch 102/500, Loss: 0.0004\n",
      "Epoch 103/500, Loss: 0.0004\n",
      "Epoch 104/500, Loss: 0.0004\n",
      "Epoch 105/500, Loss: 0.0004\n",
      "Epoch 106/500, Loss: 0.0004\n",
      "Epoch 107/500, Loss: 0.0003\n",
      "Epoch 108/500, Loss: 0.0003\n",
      "Epoch 109/500, Loss: 0.0003\n",
      "Epoch 110/500, Loss: 0.0003\n",
      "Epoch 111/500, Loss: 0.0003\n",
      "Epoch 112/500, Loss: 0.0003\n",
      "Epoch 113/500, Loss: 0.0003\n",
      "Epoch 114/500, Loss: 0.0003\n",
      "Epoch 115/500, Loss: 0.0003\n",
      "Epoch 116/500, Loss: 0.0003\n",
      "Epoch 117/500, Loss: 0.0003\n",
      "Epoch 118/500, Loss: 0.0003\n",
      "Epoch 119/500, Loss: 0.0003\n",
      "Epoch 120/500, Loss: 0.0003\n",
      "Epoch 121/500, Loss: 0.0003\n",
      "Epoch 122/500, Loss: 0.0003\n",
      "Epoch 123/500, Loss: 0.0003\n",
      "Epoch 124/500, Loss: 0.0003\n",
      "Epoch 125/500, Loss: 0.0003\n",
      "Epoch 126/500, Loss: 0.0003\n",
      "Epoch 127/500, Loss: 0.0003\n",
      "Epoch 128/500, Loss: 0.0003\n",
      "Epoch 129/500, Loss: 0.0003\n",
      "Epoch 130/500, Loss: 0.0003\n",
      "Epoch 131/500, Loss: 0.0003\n",
      "Epoch 132/500, Loss: 0.0003\n",
      "Epoch 133/500, Loss: 0.0003\n",
      "Epoch 134/500, Loss: 0.0003\n",
      "Epoch 135/500, Loss: 0.0003\n",
      "Epoch 136/500, Loss: 0.0003\n",
      "Epoch 137/500, Loss: 0.0003\n",
      "Epoch 138/500, Loss: 0.0003\n",
      "Epoch 139/500, Loss: 0.0003\n",
      "Epoch 140/500, Loss: 0.0003\n",
      "Epoch 141/500, Loss: 0.0003\n",
      "Epoch 142/500, Loss: 0.0003\n",
      "Epoch 143/500, Loss: 0.0003\n",
      "Epoch 144/500, Loss: 0.0003\n",
      "Epoch 145/500, Loss: 0.0003\n",
      "Epoch 146/500, Loss: 0.0003\n",
      "Epoch 147/500, Loss: 0.0003\n",
      "Epoch 148/500, Loss: 0.0003\n",
      "Epoch 149/500, Loss: 0.0003\n",
      "Epoch 150/500, Loss: 0.0003\n",
      "Epoch 151/500, Loss: 0.0003\n",
      "Epoch 152/500, Loss: 0.0003\n",
      "Epoch 153/500, Loss: 0.0003\n",
      "Epoch 154/500, Loss: 0.0003\n",
      "Epoch 155/500, Loss: 0.0003\n",
      "Epoch 156/500, Loss: 0.0003\n",
      "Epoch 157/500, Loss: 0.0003\n",
      "Epoch 158/500, Loss: 0.0003\n",
      "Epoch 159/500, Loss: 0.0003\n",
      "Epoch 160/500, Loss: 0.0003\n",
      "Epoch 161/500, Loss: 0.0003\n",
      "Epoch 162/500, Loss: 0.0003\n",
      "Epoch 163/500, Loss: 0.0003\n",
      "Epoch 164/500, Loss: 0.0003\n",
      "Epoch 165/500, Loss: 0.0003\n",
      "Epoch 166/500, Loss: 0.0003\n",
      "Epoch 167/500, Loss: 0.0003\n",
      "Epoch 168/500, Loss: 0.0003\n",
      "Epoch 169/500, Loss: 0.0003\n",
      "Epoch 170/500, Loss: 0.0003\n",
      "Epoch 171/500, Loss: 0.0003\n",
      "Epoch 172/500, Loss: 0.0003\n",
      "Epoch 173/500, Loss: 0.0003\n",
      "Epoch 174/500, Loss: 0.0003\n",
      "Epoch 175/500, Loss: 0.0003\n",
      "Epoch 176/500, Loss: 0.0003\n",
      "Epoch 177/500, Loss: 0.0003\n",
      "Epoch 178/500, Loss: 0.0003\n",
      "Epoch 179/500, Loss: 0.0003\n",
      "Epoch 180/500, Loss: 0.0003\n",
      "Epoch 181/500, Loss: 0.0003\n",
      "Epoch 182/500, Loss: 0.0003\n",
      "Epoch 183/500, Loss: 0.0003\n",
      "Epoch 184/500, Loss: 0.0003\n",
      "Epoch 185/500, Loss: 0.0002\n",
      "Epoch 186/500, Loss: 0.0002\n",
      "Epoch 187/500, Loss: 0.0002\n",
      "Epoch 188/500, Loss: 0.0002\n",
      "Epoch 189/500, Loss: 0.0002\n",
      "Epoch 190/500, Loss: 0.0002\n",
      "Epoch 191/500, Loss: 0.0002\n",
      "Epoch 192/500, Loss: 0.0002\n",
      "Epoch 193/500, Loss: 0.0002\n",
      "Epoch 194/500, Loss: 0.0002\n",
      "Epoch 195/500, Loss: 0.0002\n",
      "Epoch 196/500, Loss: 0.0002\n",
      "Epoch 197/500, Loss: 0.0002\n",
      "Epoch 198/500, Loss: 0.0002\n",
      "Epoch 199/500, Loss: 0.0002\n",
      "Epoch 200/500, Loss: 0.0002\n",
      "Epoch 201/500, Loss: 0.0002\n",
      "Epoch 202/500, Loss: 0.0002\n",
      "Epoch 203/500, Loss: 0.0002\n",
      "Epoch 204/500, Loss: 0.0002\n",
      "Epoch 205/500, Loss: 0.0002\n",
      "Epoch 206/500, Loss: 0.0002\n",
      "Epoch 207/500, Loss: 0.0002\n",
      "Epoch 208/500, Loss: 0.0002\n",
      "Epoch 209/500, Loss: 0.0002\n",
      "Epoch 210/500, Loss: 0.0002\n",
      "Epoch 211/500, Loss: 0.0002\n",
      "Epoch 212/500, Loss: 0.0002\n",
      "Epoch 213/500, Loss: 0.0002\n",
      "Epoch 214/500, Loss: 0.0002\n",
      "Epoch 215/500, Loss: 0.0002\n",
      "Epoch 216/500, Loss: 0.0002\n",
      "Epoch 217/500, Loss: 0.0002\n",
      "Epoch 218/500, Loss: 0.0002\n",
      "Epoch 219/500, Loss: 0.0002\n",
      "Epoch 220/500, Loss: 0.0002\n",
      "Epoch 221/500, Loss: 0.0002\n",
      "Epoch 222/500, Loss: 0.0002\n",
      "Epoch 223/500, Loss: 0.0002\n",
      "Epoch 224/500, Loss: 0.0002\n",
      "Epoch 225/500, Loss: 0.0002\n",
      "Epoch 226/500, Loss: 0.0002\n",
      "Epoch 227/500, Loss: 0.0002\n",
      "Epoch 228/500, Loss: 0.0002\n",
      "Epoch 229/500, Loss: 0.0002\n",
      "Epoch 230/500, Loss: 0.0002\n",
      "Epoch 231/500, Loss: 0.0002\n",
      "Epoch 232/500, Loss: 0.0002\n",
      "Epoch 233/500, Loss: 0.0002\n",
      "Epoch 234/500, Loss: 0.0002\n",
      "Epoch 235/500, Loss: 0.0002\n",
      "Epoch 236/500, Loss: 0.0002\n",
      "Epoch 237/500, Loss: 0.0002\n",
      "Epoch 238/500, Loss: 0.0002\n",
      "Epoch 239/500, Loss: 0.0002\n",
      "Epoch 240/500, Loss: 0.0002\n",
      "Epoch 241/500, Loss: 0.0002\n",
      "Epoch 242/500, Loss: 0.0002\n",
      "Epoch 243/500, Loss: 0.0002\n",
      "Epoch 244/500, Loss: 0.0002\n",
      "Epoch 245/500, Loss: 0.0002\n",
      "Epoch 246/500, Loss: 0.0002\n",
      "Epoch 247/500, Loss: 0.0002\n",
      "Epoch 248/500, Loss: 0.0002\n",
      "Epoch 249/500, Loss: 0.0002\n",
      "Epoch 250/500, Loss: 0.0002\n",
      "Epoch 251/500, Loss: 0.0002\n",
      "Epoch 252/500, Loss: 0.0002\n",
      "Epoch 253/500, Loss: 0.0002\n",
      "Epoch 254/500, Loss: 0.0002\n",
      "Epoch 255/500, Loss: 0.0002\n",
      "Epoch 256/500, Loss: 0.0002\n",
      "Epoch 257/500, Loss: 0.0002\n",
      "Epoch 258/500, Loss: 0.0002\n",
      "Epoch 259/500, Loss: 0.0002\n",
      "Epoch 260/500, Loss: 0.0002\n",
      "Epoch 261/500, Loss: 0.0002\n",
      "Epoch 262/500, Loss: 0.0002\n",
      "Epoch 263/500, Loss: 0.0002\n",
      "Epoch 264/500, Loss: 0.0002\n",
      "Epoch 265/500, Loss: 0.0002\n",
      "Epoch 266/500, Loss: 0.0002\n",
      "Epoch 267/500, Loss: 0.0002\n",
      "Epoch 268/500, Loss: 0.0002\n",
      "Epoch 269/500, Loss: 0.0002\n",
      "Epoch 270/500, Loss: 0.0002\n",
      "Epoch 271/500, Loss: 0.0002\n",
      "Epoch 272/500, Loss: 0.0002\n",
      "Epoch 273/500, Loss: 0.0002\n",
      "Epoch 274/500, Loss: 0.0002\n",
      "Epoch 275/500, Loss: 0.0002\n",
      "Epoch 276/500, Loss: 0.0002\n",
      "Epoch 277/500, Loss: 0.0002\n",
      "Epoch 278/500, Loss: 0.0002\n",
      "Epoch 279/500, Loss: 0.0002\n",
      "Epoch 280/500, Loss: 0.0002\n",
      "Epoch 281/500, Loss: 0.0002\n",
      "Epoch 282/500, Loss: 0.0002\n",
      "Epoch 283/500, Loss: 0.0002\n",
      "Epoch 284/500, Loss: 0.0002\n",
      "Epoch 285/500, Loss: 0.0002\n",
      "Epoch 286/500, Loss: 0.0002\n",
      "Epoch 287/500, Loss: 0.0002\n",
      "Epoch 288/500, Loss: 0.0002\n",
      "Epoch 289/500, Loss: 0.0002\n",
      "Epoch 290/500, Loss: 0.0002\n",
      "Epoch 291/500, Loss: 0.0002\n",
      "Epoch 292/500, Loss: 0.0002\n",
      "Epoch 293/500, Loss: 0.0002\n",
      "Epoch 294/500, Loss: 0.0002\n",
      "Epoch 295/500, Loss: 0.0002\n",
      "Epoch 296/500, Loss: 0.0002\n",
      "Epoch 297/500, Loss: 0.0002\n",
      "Epoch 298/500, Loss: 0.0002\n",
      "Epoch 299/500, Loss: 0.0002\n",
      "Epoch 300/500, Loss: 0.0002\n",
      "Epoch 301/500, Loss: 0.0002\n",
      "Epoch 302/500, Loss: 0.0002\n",
      "Epoch 303/500, Loss: 0.0002\n",
      "Epoch 304/500, Loss: 0.0002\n",
      "Epoch 305/500, Loss: 0.0002\n",
      "Epoch 306/500, Loss: 0.0002\n",
      "Epoch 307/500, Loss: 0.0002\n",
      "Epoch 308/500, Loss: 0.0002\n",
      "Epoch 309/500, Loss: 0.0002\n",
      "Epoch 310/500, Loss: 0.0002\n",
      "Epoch 311/500, Loss: 0.0002\n",
      "Epoch 312/500, Loss: 0.0002\n",
      "Epoch 313/500, Loss: 0.0002\n",
      "Epoch 314/500, Loss: 0.0002\n",
      "Epoch 315/500, Loss: 0.0002\n",
      "Epoch 316/500, Loss: 0.0002\n",
      "Epoch 317/500, Loss: 0.0002\n",
      "Epoch 318/500, Loss: 0.0002\n",
      "Epoch 319/500, Loss: 0.0002\n",
      "Epoch 320/500, Loss: 0.0002\n",
      "Epoch 321/500, Loss: 0.0002\n",
      "Epoch 322/500, Loss: 0.0002\n",
      "Epoch 323/500, Loss: 0.0002\n",
      "Epoch 324/500, Loss: 0.0002\n",
      "Epoch 325/500, Loss: 0.0002\n",
      "Epoch 326/500, Loss: 0.0002\n",
      "Epoch 327/500, Loss: 0.0002\n",
      "Epoch 328/500, Loss: 0.0002\n",
      "Epoch 329/500, Loss: 0.0002\n",
      "Epoch 330/500, Loss: 0.0002\n",
      "Epoch 331/500, Loss: 0.0002\n",
      "Epoch 332/500, Loss: 0.0002\n",
      "Epoch 333/500, Loss: 0.0002\n",
      "Epoch 334/500, Loss: 0.0002\n",
      "Epoch 335/500, Loss: 0.0002\n",
      "Epoch 336/500, Loss: 0.0002\n",
      "Epoch 337/500, Loss: 0.0002\n",
      "Epoch 338/500, Loss: 0.0002\n",
      "Epoch 339/500, Loss: 0.0002\n",
      "Epoch 340/500, Loss: 0.0002\n",
      "Epoch 341/500, Loss: 0.0002\n",
      "Epoch 342/500, Loss: 0.0002\n",
      "Epoch 343/500, Loss: 0.0002\n",
      "Epoch 344/500, Loss: 0.0002\n",
      "Epoch 345/500, Loss: 0.0002\n",
      "Epoch 346/500, Loss: 0.0002\n",
      "Epoch 347/500, Loss: 0.0002\n",
      "Epoch 348/500, Loss: 0.0002\n",
      "Epoch 349/500, Loss: 0.0002\n",
      "Epoch 350/500, Loss: 0.0002\n",
      "Epoch 351/500, Loss: 0.0002\n",
      "Epoch 352/500, Loss: 0.0002\n",
      "Epoch 353/500, Loss: 0.0002\n",
      "Epoch 354/500, Loss: 0.0002\n",
      "Epoch 355/500, Loss: 0.0002\n",
      "Epoch 356/500, Loss: 0.0002\n",
      "Epoch 357/500, Loss: 0.0002\n",
      "Epoch 358/500, Loss: 0.0002\n",
      "Epoch 359/500, Loss: 0.0002\n",
      "Epoch 360/500, Loss: 0.0002\n",
      "Epoch 361/500, Loss: 0.0002\n",
      "Epoch 362/500, Loss: 0.0002\n",
      "Epoch 363/500, Loss: 0.0002\n",
      "Epoch 364/500, Loss: 0.0002\n",
      "Epoch 365/500, Loss: 0.0002\n",
      "Epoch 366/500, Loss: 0.0002\n",
      "Epoch 367/500, Loss: 0.0002\n",
      "Epoch 368/500, Loss: 0.0002\n",
      "Epoch 369/500, Loss: 0.0002\n",
      "Epoch 370/500, Loss: 0.0002\n",
      "Epoch 371/500, Loss: 0.0002\n",
      "Epoch 372/500, Loss: 0.0002\n",
      "Epoch 373/500, Loss: 0.0002\n",
      "Epoch 374/500, Loss: 0.0002\n",
      "Epoch 375/500, Loss: 0.0002\n",
      "Epoch 376/500, Loss: 0.0002\n",
      "Epoch 377/500, Loss: 0.0002\n",
      "Epoch 378/500, Loss: 0.0002\n",
      "Epoch 379/500, Loss: 0.0002\n",
      "Epoch 380/500, Loss: 0.0002\n",
      "Epoch 381/500, Loss: 0.0002\n",
      "Epoch 382/500, Loss: 0.0002\n",
      "Epoch 383/500, Loss: 0.0002\n",
      "Epoch 384/500, Loss: 0.0002\n",
      "Epoch 385/500, Loss: 0.0002\n",
      "Epoch 386/500, Loss: 0.0002\n",
      "Epoch 387/500, Loss: 0.0002\n",
      "Epoch 388/500, Loss: 0.0002\n",
      "Epoch 389/500, Loss: 0.0002\n",
      "Epoch 390/500, Loss: 0.0002\n",
      "Epoch 391/500, Loss: 0.0002\n",
      "Epoch 392/500, Loss: 0.0002\n",
      "Epoch 393/500, Loss: 0.0002\n",
      "Epoch 394/500, Loss: 0.0002\n",
      "Epoch 395/500, Loss: 0.0002\n",
      "Epoch 396/500, Loss: 0.0001\n",
      "Epoch 397/500, Loss: 0.0001\n",
      "Epoch 398/500, Loss: 0.0001\n",
      "Epoch 399/500, Loss: 0.0001\n",
      "Epoch 400/500, Loss: 0.0001\n",
      "Epoch 401/500, Loss: 0.0001\n",
      "Epoch 402/500, Loss: 0.0001\n",
      "Epoch 403/500, Loss: 0.0001\n",
      "Epoch 404/500, Loss: 0.0001\n",
      "Epoch 405/500, Loss: 0.0001\n",
      "Epoch 406/500, Loss: 0.0001\n",
      "Epoch 407/500, Loss: 0.0001\n",
      "Epoch 408/500, Loss: 0.0001\n",
      "Epoch 409/500, Loss: 0.0001\n",
      "Epoch 410/500, Loss: 0.0001\n",
      "Epoch 411/500, Loss: 0.0001\n",
      "Epoch 412/500, Loss: 0.0001\n",
      "Epoch 413/500, Loss: 0.0001\n",
      "Epoch 414/500, Loss: 0.0001\n",
      "Epoch 415/500, Loss: 0.0001\n",
      "Epoch 416/500, Loss: 0.0001\n",
      "Epoch 417/500, Loss: 0.0001\n",
      "Epoch 418/500, Loss: 0.0001\n",
      "Epoch 419/500, Loss: 0.0001\n",
      "Epoch 420/500, Loss: 0.0001\n",
      "Epoch 421/500, Loss: 0.0001\n",
      "Epoch 422/500, Loss: 0.0001\n",
      "Epoch 423/500, Loss: 0.0001\n",
      "Epoch 424/500, Loss: 0.0001\n",
      "Epoch 425/500, Loss: 0.0001\n",
      "Epoch 426/500, Loss: 0.0001\n",
      "Epoch 427/500, Loss: 0.0001\n",
      "Epoch 428/500, Loss: 0.0001\n",
      "Epoch 429/500, Loss: 0.0001\n",
      "Epoch 430/500, Loss: 0.0001\n",
      "Epoch 431/500, Loss: 0.0001\n",
      "Epoch 432/500, Loss: 0.0001\n",
      "Epoch 433/500, Loss: 0.0001\n",
      "Epoch 434/500, Loss: 0.0001\n",
      "Epoch 435/500, Loss: 0.0001\n",
      "Epoch 436/500, Loss: 0.0001\n",
      "Epoch 437/500, Loss: 0.0001\n",
      "Epoch 438/500, Loss: 0.0001\n",
      "Epoch 439/500, Loss: 0.0001\n",
      "Epoch 440/500, Loss: 0.0001\n",
      "Epoch 441/500, Loss: 0.0001\n",
      "Epoch 442/500, Loss: 0.0001\n",
      "Epoch 443/500, Loss: 0.0001\n",
      "Epoch 444/500, Loss: 0.0001\n",
      "Epoch 445/500, Loss: 0.0001\n",
      "Epoch 446/500, Loss: 0.0001\n",
      "Epoch 447/500, Loss: 0.0001\n",
      "Epoch 448/500, Loss: 0.0001\n",
      "Epoch 449/500, Loss: 0.0001\n",
      "Epoch 450/500, Loss: 0.0001\n",
      "Epoch 451/500, Loss: 0.0001\n",
      "Epoch 452/500, Loss: 0.0001\n",
      "Epoch 453/500, Loss: 0.0001\n",
      "Epoch 454/500, Loss: 0.0001\n",
      "Epoch 455/500, Loss: 0.0001\n",
      "Epoch 456/500, Loss: 0.0001\n",
      "Epoch 457/500, Loss: 0.0001\n",
      "Epoch 458/500, Loss: 0.0001\n",
      "Epoch 459/500, Loss: 0.0001\n",
      "Epoch 460/500, Loss: 0.0001\n",
      "Epoch 461/500, Loss: 0.0001\n",
      "Epoch 462/500, Loss: 0.0001\n",
      "Epoch 463/500, Loss: 0.0001\n",
      "Epoch 464/500, Loss: 0.0001\n",
      "Epoch 465/500, Loss: 0.0001\n",
      "Epoch 466/500, Loss: 0.0001\n",
      "Epoch 467/500, Loss: 0.0001\n",
      "Epoch 468/500, Loss: 0.0001\n",
      "Epoch 469/500, Loss: 0.0001\n",
      "Epoch 470/500, Loss: 0.0001\n",
      "Epoch 471/500, Loss: 0.0001\n",
      "Epoch 472/500, Loss: 0.0001\n",
      "Epoch 473/500, Loss: 0.0001\n",
      "Epoch 474/500, Loss: 0.0001\n",
      "Epoch 475/500, Loss: 0.0001\n",
      "Epoch 476/500, Loss: 0.0001\n",
      "Epoch 477/500, Loss: 0.0001\n",
      "Epoch 478/500, Loss: 0.0001\n",
      "Epoch 479/500, Loss: 0.0001\n",
      "Epoch 480/500, Loss: 0.0001\n",
      "Epoch 481/500, Loss: 0.0001\n",
      "Epoch 482/500, Loss: 0.0001\n",
      "Epoch 483/500, Loss: 0.0001\n",
      "Epoch 484/500, Loss: 0.0001\n",
      "Epoch 485/500, Loss: 0.0001\n",
      "Epoch 486/500, Loss: 0.0001\n",
      "Epoch 487/500, Loss: 0.0001\n",
      "Epoch 488/500, Loss: 0.0001\n",
      "Epoch 489/500, Loss: 0.0001\n",
      "Epoch 490/500, Loss: 0.0001\n",
      "Epoch 491/500, Loss: 0.0001\n",
      "Epoch 492/500, Loss: 0.0001\n",
      "Epoch 493/500, Loss: 0.0001\n",
      "Epoch 494/500, Loss: 0.0001\n",
      "Epoch 495/500, Loss: 0.0001\n",
      "Epoch 496/500, Loss: 0.0001\n",
      "Epoch 497/500, Loss: 0.0001\n",
      "Epoch 498/500, Loss: 0.0001\n",
      "Epoch 499/500, Loss: 0.0001\n",
      "Epoch 500/500, Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "checkpoint_folder = \"/Users/nimitt/Documents/DigitalSystems/Project/Model_States\"\n",
    "model_filename = f\"model_{block_size}_{hidden_size}_{text_len}.pt\"\n",
    "model_path = os.path.join(checkpoint_folder, model_filename)\n",
    "# Ensure the folder exists\n",
    "os.makedirs(checkpoint_folder, exist_ok=True)\n",
    "\n",
    "model.train(X,Y,500,0.01,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "def convert_to_X(prompt):\n",
    "    X_ = np.zeros((len(prompt),input_size))\n",
    "    for i in range(len(prompt)):\n",
    "        X_[i][stoi[prompt[i]]] = 1\n",
    "    return torch.tensor(X_,dtype = torch.float32)\n",
    "        \n",
    "prompt = \"alumni\"\n",
    "max_len = 100\n",
    "context = []\n",
    "for j in range(len(prompt)):\n",
    "    context = context + [stoi[prompt[j]]]\n",
    "context = context[-block_size:]\n",
    "\n",
    "generated_text = prompt\n",
    "for i in range(max_len):\n",
    "    x = convert_to_X(generated_text)\n",
    "    y_pred = model(x)[-1]\n",
    "    # ix = torch.distributions.categorical.Categorical(logits=y_pred.squeeze()).sample()\n",
    "    ix = torch.argmax(y_pred)\n",
    "    ch = itos[ix.item()]\n",
    "    generated_text += ch\n",
    "    context = context[1:] + [ix]\n",
    "\n",
    "genrated_text = generated_text.replace('|','\\n')\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alumni relations advisory 02 (version 1.0, december 2021)                                 page 1 of 1 ~ indian institute of technology gandhinagar  ~ ~honorary  alumni program at iit gandhinagar  ~(as approved by the bog in its 33rd meeting held on 15 november 2021)  ~ ~ ~the board of governors of the institute in its 33rd meeting held on 15 november ~2021 approved the following honorary alumni program  at iit gandhinagar.  ~  ~1. individuals who are not graduates of iit gandhinagar and who make'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5093],\n",
      "        [ 0.7556],\n",
      "        [-0.6410],\n",
      "        [ 0.8547],\n",
      "        [-0.7428],\n",
      "        [-0.5850],\n",
      "        [-0.4661],\n",
      "        [ 0.6631],\n",
      "        [ 0.4788],\n",
      "        [ 0.7129],\n",
      "        [ 0.1082],\n",
      "        [ 0.7051],\n",
      "        [ 0.7556],\n",
      "        [ 0.0182],\n",
      "        [-0.3916],\n",
      "        [ 0.4623],\n",
      "        [-0.9483],\n",
      "        [ 0.1123],\n",
      "        [-0.7316],\n",
      "        [ 0.5437],\n",
      "        [-0.5749],\n",
      "        [-0.7134],\n",
      "        [-0.8074],\n",
      "        [ 0.5112],\n",
      "        [-0.9066],\n",
      "        [ 0.7032],\n",
      "        [ 0.5394],\n",
      "        [ 0.1768],\n",
      "        [-0.3579],\n",
      "        [ 0.9082],\n",
      "        [-0.7271],\n",
      "        [-0.5752],\n",
      "        [-0.1813],\n",
      "        [ 0.3694],\n",
      "        [ 0.6758],\n",
      "        [-0.6852],\n",
      "        [-0.8166],\n",
      "        [-0.2973],\n",
      "        [-0.3089],\n",
      "        [-0.6119],\n",
      "        [ 0.6855],\n",
      "        [ 0.9560],\n",
      "        [-0.8068],\n",
      "        [ 0.1511],\n",
      "        [ 0.6658],\n",
      "        [-0.0310],\n",
      "        [-0.1576],\n",
      "        [-0.7469],\n",
      "        [ 0.9593],\n",
      "        [ 0.9472]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9783],\n",
      "        [ 0.9320],\n",
      "        [-0.7514],\n",
      "        [ 0.3109],\n",
      "        [-0.6069],\n",
      "        [-0.8943],\n",
      "        [-0.9901],\n",
      "        [-0.0289],\n",
      "        [ 0.9497],\n",
      "        [ 0.3746],\n",
      "        [-0.1543],\n",
      "        [ 0.1450],\n",
      "        [ 0.9780],\n",
      "        [ 0.8229],\n",
      "        [-0.9771],\n",
      "        [ 0.9205],\n",
      "        [-0.1743],\n",
      "        [-0.1620],\n",
      "        [-0.4409],\n",
      "        [ 0.8963],\n",
      "        [-0.9671],\n",
      "        [-0.9831],\n",
      "        [ 0.8648],\n",
      "        [-0.1100],\n",
      "        [ 0.4111],\n",
      "        [ 0.9884],\n",
      "        [-0.8814],\n",
      "        [-0.9250],\n",
      "        [-0.4239],\n",
      "        [ 0.8789],\n",
      "        [-0.8635],\n",
      "        [-0.9456],\n",
      "        [-0.9964],\n",
      "        [-0.8536],\n",
      "        [ 0.6270],\n",
      "        [-0.7514],\n",
      "        [-0.7093],\n",
      "        [-0.8746],\n",
      "        [-0.9444],\n",
      "        [-0.8564],\n",
      "        [-0.9977],\n",
      "        [ 0.9328],\n",
      "        [ 0.0232],\n",
      "        [ 0.5062],\n",
      "        [ 0.6098],\n",
      "        [ 0.5694],\n",
      "        [-0.9651],\n",
      "        [-0.7381],\n",
      "        [ 0.4536],\n",
      "        [ 0.9238]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9990],\n",
      "        [ 0.8945],\n",
      "        [-0.9971],\n",
      "        [ 0.7784],\n",
      "        [-0.9997],\n",
      "        [-0.9976],\n",
      "        [-0.9970],\n",
      "        [ 0.8264],\n",
      "        [-0.8565],\n",
      "        [ 0.9441],\n",
      "        [ 0.5852],\n",
      "        [ 0.9801],\n",
      "        [ 0.5644],\n",
      "        [ 0.9704],\n",
      "        [-0.9849],\n",
      "        [ 0.9965],\n",
      "        [ 0.5304],\n",
      "        [ 0.4130],\n",
      "        [ 0.6738],\n",
      "        [ 0.0360],\n",
      "        [-0.9782],\n",
      "        [-0.9658],\n",
      "        [-0.9987],\n",
      "        [ 0.5730],\n",
      "        [ 0.7793],\n",
      "        [ 0.9953],\n",
      "        [ 0.1945],\n",
      "        [-0.9978],\n",
      "        [-0.7020],\n",
      "        [ 0.9997],\n",
      "        [-0.8936],\n",
      "        [-0.9992],\n",
      "        [-0.9983],\n",
      "        [ 0.8612],\n",
      "        [ 0.9631],\n",
      "        [ 0.7727],\n",
      "        [-0.9099],\n",
      "        [-0.9998],\n",
      "        [-0.9741],\n",
      "        [-0.9996],\n",
      "        [-0.5557],\n",
      "        [ 0.9208],\n",
      "        [-0.9722],\n",
      "        [-0.6324],\n",
      "        [ 0.9904],\n",
      "        [-0.5255],\n",
      "        [-0.9995],\n",
      "        [-0.6805],\n",
      "        [ 0.6790],\n",
      "        [ 0.9982]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9956],\n",
      "        [-0.1391],\n",
      "        [-0.9936],\n",
      "        [ 0.8339],\n",
      "        [-0.9762],\n",
      "        [-0.9863],\n",
      "        [-0.9920],\n",
      "        [ 0.9805],\n",
      "        [-0.9704],\n",
      "        [ 0.9822],\n",
      "        [-0.5361],\n",
      "        [-0.5374],\n",
      "        [-0.0241],\n",
      "        [ 0.7925],\n",
      "        [-0.9661],\n",
      "        [ 0.9745],\n",
      "        [-0.3110],\n",
      "        [ 0.9881],\n",
      "        [-0.9756],\n",
      "        [-0.8394],\n",
      "        [-0.9986],\n",
      "        [-0.9919],\n",
      "        [-0.3498],\n",
      "        [ 0.9021],\n",
      "        [ 0.4714],\n",
      "        [ 0.9995],\n",
      "        [-0.9161],\n",
      "        [-0.9896],\n",
      "        [-0.9189],\n",
      "        [ 0.7990],\n",
      "        [-0.9778],\n",
      "        [-0.9999],\n",
      "        [-1.0000],\n",
      "        [ 0.1665],\n",
      "        [ 0.9325],\n",
      "        [-0.9951],\n",
      "        [-0.5723],\n",
      "        [-0.9999],\n",
      "        [-1.0000],\n",
      "        [-0.9987],\n",
      "        [-0.9460],\n",
      "        [ 0.0682],\n",
      "        [-0.9961],\n",
      "        [-0.0335],\n",
      "        [ 0.9968],\n",
      "        [-0.6516],\n",
      "        [-0.9991],\n",
      "        [-0.5546],\n",
      "        [-0.9807],\n",
      "        [ 0.9982]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9995],\n",
      "        [ 0.8867],\n",
      "        [ 0.4129],\n",
      "        [-0.4648],\n",
      "        [-0.9333],\n",
      "        [-0.9969],\n",
      "        [-0.9964],\n",
      "        [-0.8876],\n",
      "        [ 0.3138],\n",
      "        [ 0.7900],\n",
      "        [-0.9952],\n",
      "        [ 0.9884],\n",
      "        [ 0.8059],\n",
      "        [ 0.9701],\n",
      "        [-0.7770],\n",
      "        [ 0.9977],\n",
      "        [ 0.8396],\n",
      "        [ 0.8941],\n",
      "        [-0.8382],\n",
      "        [-0.4394],\n",
      "        [-0.9981],\n",
      "        [-0.8095],\n",
      "        [-0.5449],\n",
      "        [-0.9762],\n",
      "        [ 0.7467],\n",
      "        [ 0.9989],\n",
      "        [ 0.2794],\n",
      "        [-0.9990],\n",
      "        [ 0.6983],\n",
      "        [ 0.7381],\n",
      "        [-0.9402],\n",
      "        [-1.0000],\n",
      "        [-0.9982],\n",
      "        [-0.7857],\n",
      "        [ 0.3560],\n",
      "        [-0.8899],\n",
      "        [-0.0536],\n",
      "        [-0.9998],\n",
      "        [-0.9671],\n",
      "        [-0.9982],\n",
      "        [-0.9914],\n",
      "        [-0.3217],\n",
      "        [ 0.4823],\n",
      "        [-0.9650],\n",
      "        [ 0.9975],\n",
      "        [-0.9536],\n",
      "        [-0.9985],\n",
      "        [ 0.9938],\n",
      "        [-0.4296],\n",
      "        [ 0.9975]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9992],\n",
      "        [-0.6320],\n",
      "        [-0.3900],\n",
      "        [-0.7257],\n",
      "        [-0.9968],\n",
      "        [-0.9918],\n",
      "        [-0.9978],\n",
      "        [ 0.5653],\n",
      "        [ 0.2238],\n",
      "        [ 0.9942],\n",
      "        [ 0.1620],\n",
      "        [ 0.9992],\n",
      "        [ 0.9998],\n",
      "        [ 0.9994],\n",
      "        [-0.2525],\n",
      "        [ 0.9424],\n",
      "        [-0.9278],\n",
      "        [ 0.7089],\n",
      "        [ 0.8223],\n",
      "        [ 0.2262],\n",
      "        [-1.0000],\n",
      "        [ 0.9004],\n",
      "        [-0.9993],\n",
      "        [-0.6626],\n",
      "        [-0.9906],\n",
      "        [ 0.9991],\n",
      "        [ 0.9401],\n",
      "        [-0.7126],\n",
      "        [ 0.5709],\n",
      "        [-0.0706],\n",
      "        [-0.9957],\n",
      "        [-0.9934],\n",
      "        [-0.7834],\n",
      "        [-0.4574],\n",
      "        [ 0.9997],\n",
      "        [-0.6693],\n",
      "        [-0.8989],\n",
      "        [-0.9992],\n",
      "        [-0.9582],\n",
      "        [-0.9931],\n",
      "        [ 0.2815],\n",
      "        [ 0.9253],\n",
      "        [-0.1276],\n",
      "        [-0.5933],\n",
      "        [ 0.1569],\n",
      "        [-0.9551],\n",
      "        [-0.9997],\n",
      "        [-0.9904],\n",
      "        [ 0.7835],\n",
      "        [ 0.9896]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9997],\n",
      "        [ 0.3515],\n",
      "        [-0.9521],\n",
      "        [-0.2416],\n",
      "        [-0.9183],\n",
      "        [-0.9956],\n",
      "        [-0.9997],\n",
      "        [ 0.8053],\n",
      "        [ 0.9872],\n",
      "        [ 0.9952],\n",
      "        [ 0.6670],\n",
      "        [ 0.9545],\n",
      "        [ 0.9970],\n",
      "        [ 0.9900],\n",
      "        [-0.9940],\n",
      "        [ 0.9904],\n",
      "        [-0.9206],\n",
      "        [-0.8126],\n",
      "        [ 0.8327],\n",
      "        [-0.9120],\n",
      "        [-0.9972],\n",
      "        [-0.4632],\n",
      "        [-0.9928],\n",
      "        [ 0.1907],\n",
      "        [ 0.4393],\n",
      "        [ 0.9995],\n",
      "        [-0.9591],\n",
      "        [-0.9986],\n",
      "        [ 0.0746],\n",
      "        [-0.8476],\n",
      "        [-0.8793],\n",
      "        [-0.9968],\n",
      "        [-1.0000],\n",
      "        [ 0.2066],\n",
      "        [ 0.8617],\n",
      "        [-0.1363],\n",
      "        [ 0.0530],\n",
      "        [-0.9989],\n",
      "        [-0.9965],\n",
      "        [-0.9902],\n",
      "        [-0.9899],\n",
      "        [-0.1941],\n",
      "        [-0.6856],\n",
      "        [ 0.2232],\n",
      "        [ 0.9838],\n",
      "        [-0.9656],\n",
      "        [-0.9999],\n",
      "        [-0.9519],\n",
      "        [-0.7084],\n",
      "        [ 0.9969]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9998],\n",
      "        [ 0.6498],\n",
      "        [-0.1273],\n",
      "        [ 0.9281],\n",
      "        [-0.9456],\n",
      "        [-0.9974],\n",
      "        [-0.9995],\n",
      "        [-0.7046],\n",
      "        [ 0.9801],\n",
      "        [-0.4236],\n",
      "        [-0.9185],\n",
      "        [-0.8915],\n",
      "        [ 0.9798],\n",
      "        [ 0.9727],\n",
      "        [-0.7301],\n",
      "        [ 0.9983],\n",
      "        [ 0.8497],\n",
      "        [-0.1249],\n",
      "        [-0.8339],\n",
      "        [ 0.4143],\n",
      "        [-0.9808],\n",
      "        [-0.9364],\n",
      "        [-0.9275],\n",
      "        [ 0.9585],\n",
      "        [ 0.8306],\n",
      "        [ 0.9901],\n",
      "        [ 0.0082],\n",
      "        [-0.9060],\n",
      "        [-0.7631],\n",
      "        [-0.0195],\n",
      "        [-0.9356],\n",
      "        [-0.9953],\n",
      "        [-0.9969],\n",
      "        [-0.2403],\n",
      "        [ 0.0924],\n",
      "        [-0.1738],\n",
      "        [ 0.3648],\n",
      "        [-0.9997],\n",
      "        [-0.9986],\n",
      "        [-0.9742],\n",
      "        [-0.8226],\n",
      "        [-0.9957],\n",
      "        [-0.2648],\n",
      "        [-0.2384],\n",
      "        [ 0.9999],\n",
      "        [-0.9974],\n",
      "        [-0.9992],\n",
      "        [-0.0250],\n",
      "        [-0.8315],\n",
      "        [ 0.9660]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 9.9476e-01],\n",
      "        [-9.6128e-01],\n",
      "        [-6.8647e-01],\n",
      "        [ 2.6282e-01],\n",
      "        [-9.9963e-01],\n",
      "        [-8.2128e-01],\n",
      "        [-9.9154e-01],\n",
      "        [ 7.0775e-01],\n",
      "        [-9.7224e-01],\n",
      "        [ 7.2105e-01],\n",
      "        [-1.0455e-01],\n",
      "        [ 9.9137e-01],\n",
      "        [ 7.3044e-01],\n",
      "        [ 9.8728e-01],\n",
      "        [ 5.4989e-01],\n",
      "        [ 9.8843e-01],\n",
      "        [-7.9044e-01],\n",
      "        [ 2.2245e-02],\n",
      "        [ 9.7045e-01],\n",
      "        [ 8.5786e-01],\n",
      "        [-9.6707e-01],\n",
      "        [ 2.1511e-02],\n",
      "        [-9.6947e-01],\n",
      "        [-9.9593e-01],\n",
      "        [-9.7410e-01],\n",
      "        [ 9.9770e-01],\n",
      "        [ 5.4849e-01],\n",
      "        [-9.1233e-01],\n",
      "        [ 3.2065e-01],\n",
      "        [ 5.0059e-01],\n",
      "        [-9.9093e-01],\n",
      "        [-9.9365e-01],\n",
      "        [ 2.4156e-02],\n",
      "        [ 1.6513e-04],\n",
      "        [ 9.9927e-01],\n",
      "        [-6.7037e-01],\n",
      "        [-9.6663e-01],\n",
      "        [-9.9972e-01],\n",
      "        [-8.9242e-01],\n",
      "        [-9.9119e-01],\n",
      "        [-9.9070e-01],\n",
      "        [ 5.3465e-01],\n",
      "        [ 2.6796e-01],\n",
      "        [-1.3428e-02],\n",
      "        [ 3.9550e-01],\n",
      "        [-9.5279e-01],\n",
      "        [-9.9915e-01],\n",
      "        [-7.8435e-01],\n",
      "        [ 8.6974e-01],\n",
      "        [ 9.6092e-01]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9989],\n",
      "        [ 0.4711],\n",
      "        [-0.4004],\n",
      "        [ 0.0296],\n",
      "        [-0.9829],\n",
      "        [-0.9188],\n",
      "        [-0.9998],\n",
      "        [ 0.4689],\n",
      "        [ 0.9560],\n",
      "        [ 0.6956],\n",
      "        [ 0.9829],\n",
      "        [-0.0463],\n",
      "        [ 0.8738],\n",
      "        [ 0.2705],\n",
      "        [ 0.2440],\n",
      "        [ 0.9787],\n",
      "        [-0.0083],\n",
      "        [ 0.5083],\n",
      "        [ 0.2772],\n",
      "        [ 0.6079],\n",
      "        [-0.9291],\n",
      "        [-0.9957],\n",
      "        [ 0.2922],\n",
      "        [ 0.6765],\n",
      "        [ 0.0332],\n",
      "        [ 0.9970],\n",
      "        [-0.3444],\n",
      "        [-0.8398],\n",
      "        [-0.7077],\n",
      "        [ 0.7966],\n",
      "        [-0.9992],\n",
      "        [-0.9848],\n",
      "        [-1.0000],\n",
      "        [-0.7668],\n",
      "        [ 0.9985],\n",
      "        [-0.7679],\n",
      "        [-0.2887],\n",
      "        [-0.9993],\n",
      "        [-0.9989],\n",
      "        [-0.9980],\n",
      "        [-0.9944],\n",
      "        [ 0.2308],\n",
      "        [ 0.2845],\n",
      "        [ 0.9437],\n",
      "        [ 0.3793],\n",
      "        [-0.1853],\n",
      "        [-0.9999],\n",
      "        [ 0.4309],\n",
      "        [-0.8114],\n",
      "        [ 0.9877]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9985],\n",
      "        [ 0.8663],\n",
      "        [-0.9862],\n",
      "        [ 0.9493],\n",
      "        [-0.9978],\n",
      "        [-0.9208],\n",
      "        [-0.9995],\n",
      "        [ 0.9206],\n",
      "        [ 0.7402],\n",
      "        [ 0.9826],\n",
      "        [ 0.8369],\n",
      "        [ 0.8748],\n",
      "        [ 0.9280],\n",
      "        [ 0.9405],\n",
      "        [-0.3820],\n",
      "        [ 0.9751],\n",
      "        [-0.9922],\n",
      "        [ 0.6749],\n",
      "        [-0.3538],\n",
      "        [ 0.1730],\n",
      "        [-0.9996],\n",
      "        [-0.7155],\n",
      "        [-0.9997],\n",
      "        [-0.8981],\n",
      "        [-0.6610],\n",
      "        [ 0.9983],\n",
      "        [ 0.3424],\n",
      "        [-0.9954],\n",
      "        [-0.5366],\n",
      "        [ 0.9779],\n",
      "        [-0.9386],\n",
      "        [-0.9934],\n",
      "        [-0.9411],\n",
      "        [ 0.7635],\n",
      "        [ 0.9958],\n",
      "        [-0.9768],\n",
      "        [-0.9575],\n",
      "        [-0.9993],\n",
      "        [-0.9992],\n",
      "        [-0.9899],\n",
      "        [-0.8610],\n",
      "        [ 0.9526],\n",
      "        [-0.6868],\n",
      "        [ 0.2521],\n",
      "        [ 0.9998],\n",
      "        [-0.9455],\n",
      "        [-0.9983],\n",
      "        [-0.9118],\n",
      "        [ 0.9946],\n",
      "        [ 0.9932]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9989],\n",
      "        [ 0.3290],\n",
      "        [ 0.7289],\n",
      "        [-0.6172],\n",
      "        [-0.3669],\n",
      "        [-0.9914],\n",
      "        [-0.9994],\n",
      "        [ 0.6964],\n",
      "        [-0.2329],\n",
      "        [ 0.3043],\n",
      "        [ 0.9763],\n",
      "        [-0.9189],\n",
      "        [-0.8157],\n",
      "        [ 0.9256],\n",
      "        [-0.6997],\n",
      "        [ 0.9971],\n",
      "        [-0.9044],\n",
      "        [-0.9776],\n",
      "        [-0.0704],\n",
      "        [-0.5237],\n",
      "        [-0.9996],\n",
      "        [-0.3637],\n",
      "        [ 0.0702],\n",
      "        [-0.0345],\n",
      "        [ 0.9111],\n",
      "        [ 0.9993],\n",
      "        [-0.3376],\n",
      "        [-0.9998],\n",
      "        [-0.3341],\n",
      "        [ 0.5843],\n",
      "        [-0.9375],\n",
      "        [-0.9986],\n",
      "        [-0.9999],\n",
      "        [-0.7041],\n",
      "        [ 0.9719],\n",
      "        [-0.9569],\n",
      "        [-0.8688],\n",
      "        [-1.0000],\n",
      "        [-0.9995],\n",
      "        [-0.9961],\n",
      "        [-0.8724],\n",
      "        [ 0.7917],\n",
      "        [-0.2314],\n",
      "        [ 0.6047],\n",
      "        [ 0.9974],\n",
      "        [-0.9832],\n",
      "        [-0.9996],\n",
      "        [ 0.3851],\n",
      "        [-0.0090],\n",
      "        [ 1.0000]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9964],\n",
      "        [ 0.0219],\n",
      "        [-0.9973],\n",
      "        [ 0.8991],\n",
      "        [-0.9925],\n",
      "        [-0.9993],\n",
      "        [-0.9993],\n",
      "        [ 0.9569],\n",
      "        [-0.6849],\n",
      "        [ 0.9639],\n",
      "        [ 0.3795],\n",
      "        [ 0.8940],\n",
      "        [ 0.9999],\n",
      "        [ 0.9991],\n",
      "        [ 0.9783],\n",
      "        [ 0.9877],\n",
      "        [-0.4077],\n",
      "        [ 0.9935],\n",
      "        [-0.9019],\n",
      "        [-0.9101],\n",
      "        [-0.9996],\n",
      "        [-0.8309],\n",
      "        [-0.9934],\n",
      "        [-0.9964],\n",
      "        [-0.9659],\n",
      "        [ 0.9972],\n",
      "        [ 0.9973],\n",
      "        [-0.9893],\n",
      "        [ 0.4688],\n",
      "        [ 0.9767],\n",
      "        [-0.9946],\n",
      "        [-0.9999],\n",
      "        [-0.9986],\n",
      "        [ 0.9594],\n",
      "        [ 0.9995],\n",
      "        [-0.4695],\n",
      "        [ 0.9141],\n",
      "        [-0.9997],\n",
      "        [-0.9690],\n",
      "        [-0.9949],\n",
      "        [ 0.8144],\n",
      "        [-0.7737],\n",
      "        [ 0.9725],\n",
      "        [-0.9576],\n",
      "        [ 0.9999],\n",
      "        [-0.9926],\n",
      "        [-0.9996],\n",
      "        [-0.1710],\n",
      "        [-0.5386],\n",
      "        [ 0.9975]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9905],\n",
      "        [-0.7094],\n",
      "        [-0.8011],\n",
      "        [ 0.9712],\n",
      "        [-0.9961],\n",
      "        [-0.9694],\n",
      "        [-0.9978],\n",
      "        [ 0.2747],\n",
      "        [-0.5638],\n",
      "        [ 0.3977],\n",
      "        [-0.7851],\n",
      "        [ 0.7446],\n",
      "        [ 0.9727],\n",
      "        [ 0.9998],\n",
      "        [-0.8841],\n",
      "        [ 0.9985],\n",
      "        [-0.8842],\n",
      "        [-0.9241],\n",
      "        [ 0.2204],\n",
      "        [ 0.0851],\n",
      "        [-1.0000],\n",
      "        [ 0.9508],\n",
      "        [-0.9349],\n",
      "        [-0.7527],\n",
      "        [ 0.3823],\n",
      "        [ 0.9903],\n",
      "        [ 0.0285],\n",
      "        [-0.8341],\n",
      "        [-0.6769],\n",
      "        [-0.0567],\n",
      "        [-0.7976],\n",
      "        [-0.9338],\n",
      "        [-0.9931],\n",
      "        [ 0.1589],\n",
      "        [-0.3531],\n",
      "        [-0.8926],\n",
      "        [-0.7910],\n",
      "        [-0.6303],\n",
      "        [-0.9668],\n",
      "        [-0.6890],\n",
      "        [ 0.3524],\n",
      "        [ 0.8105],\n",
      "        [-0.8891],\n",
      "        [ 0.1941],\n",
      "        [ 0.8358],\n",
      "        [-0.9534],\n",
      "        [-0.8082],\n",
      "        [ 0.0171],\n",
      "        [ 0.9887],\n",
      "        [ 0.9993]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9988],\n",
      "        [ 0.0690],\n",
      "        [ 0.2563],\n",
      "        [-0.7983],\n",
      "        [-0.9991],\n",
      "        [-0.9908],\n",
      "        [-0.9953],\n",
      "        [-0.9907],\n",
      "        [ 0.5204],\n",
      "        [ 0.9738],\n",
      "        [-0.9743],\n",
      "        [ 0.9628],\n",
      "        [ 0.9760],\n",
      "        [ 0.3396],\n",
      "        [-0.8611],\n",
      "        [ 0.9982],\n",
      "        [ 0.3630],\n",
      "        [ 0.6786],\n",
      "        [-0.8650],\n",
      "        [-0.7059],\n",
      "        [-0.9994],\n",
      "        [-0.5566],\n",
      "        [-0.6291],\n",
      "        [ 0.3160],\n",
      "        [-0.7935],\n",
      "        [ 0.9974],\n",
      "        [ 0.9339],\n",
      "        [-0.9949],\n",
      "        [-0.3741],\n",
      "        [ 0.9235],\n",
      "        [-0.9999],\n",
      "        [-0.9967],\n",
      "        [-1.0000],\n",
      "        [-0.8944],\n",
      "        [ 0.6453],\n",
      "        [ 0.3134],\n",
      "        [ 0.7609],\n",
      "        [-0.9986],\n",
      "        [-0.9362],\n",
      "        [-0.9988],\n",
      "        [-0.9478],\n",
      "        [-0.7107],\n",
      "        [ 0.6032],\n",
      "        [ 0.9871],\n",
      "        [ 0.9879],\n",
      "        [ 0.4866],\n",
      "        [-0.9925],\n",
      "        [ 0.9893],\n",
      "        [-0.3049],\n",
      "        [ 0.9998]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.9876],\n",
      "        [-0.9417],\n",
      "        [-0.7073],\n",
      "        [-0.9578],\n",
      "        [-0.9987],\n",
      "        [-0.6164],\n",
      "        [-0.9951],\n",
      "        [-0.5880],\n",
      "        [ 0.9914],\n",
      "        [ 0.9493],\n",
      "        [ 0.9946],\n",
      "        [ 0.4845],\n",
      "        [ 0.9793],\n",
      "        [ 0.9858],\n",
      "        [-0.7177],\n",
      "        [ 0.7667],\n",
      "        [ 0.0758],\n",
      "        [-0.1999],\n",
      "        [ 0.9709],\n",
      "        [-0.1179],\n",
      "        [-0.9046],\n",
      "        [ 0.3660],\n",
      "        [-0.9999],\n",
      "        [-0.9925],\n",
      "        [-0.9620],\n",
      "        [ 0.9809],\n",
      "        [-0.9314],\n",
      "        [-0.9969],\n",
      "        [ 0.4328],\n",
      "        [ 0.1143],\n",
      "        [-0.9951],\n",
      "        [-0.9396],\n",
      "        [ 0.2882],\n",
      "        [-0.9465],\n",
      "        [ 0.9999],\n",
      "        [-0.6821],\n",
      "        [-0.1835],\n",
      "        [-0.9957],\n",
      "        [-0.9597],\n",
      "        [-0.9781],\n",
      "        [-0.9886],\n",
      "        [ 0.9647],\n",
      "        [ 0.1667],\n",
      "        [ 0.9943],\n",
      "        [ 0.6572],\n",
      "        [-0.9966],\n",
      "        [-0.9989],\n",
      "        [-0.8808],\n",
      "        [-0.9647],\n",
      "        [ 0.9900]], grad_fn=<TanhBackward0>)\n",
      "alumni relations \n"
     ]
    }
   ],
   "source": [
    "model_read = LSTM(input_size+hidden_size, hidden_size, input_size)\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "model_path = \"/Users/nimitt/Documents/DigitalSystems/Project/Model_States/model_10_50_5000.pt\"\n",
    "checkpoint = torch.load(model_path,map_location=torch.device('cpu'))\n",
    "model_read.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Testing\n",
    "\n",
    "def convert_to_X(prompt):\n",
    "    X_ = np.zeros((len(prompt),input_size))\n",
    "    for i in range(len(prompt)):\n",
    "        X_[i][stoi[prompt[i]]] = 1\n",
    "    return torch.tensor(X_,dtype = torch.float32)\n",
    "        \n",
    "prompt = \"alumni relations\"\n",
    "max_len = 1\n",
    "context = []\n",
    "for j in range(len(prompt)):\n",
    "    context = context + [stoi[prompt[j]]]\n",
    "context = context[-block_size:]\n",
    "\n",
    "generated_text = prompt\n",
    "for i in range(max_len):\n",
    "    x = convert_to_X(generated_text)\n",
    "    y_pred = model_read(x)[-1]\n",
    "    # ix = torch.distributions.categorical.Categorical(logits=y_pred.squeeze()).sample()\n",
    "    ix = torch.argmax(y_pred)\n",
    "    ch = itos[ix.item()]\n",
    "    generated_text += ch\n",
    "    context = context[1:] + [ix]\n",
    "\n",
    "genrated_text = generated_text.replace('|','\\n')\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
